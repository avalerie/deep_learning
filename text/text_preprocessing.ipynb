{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9199de1f-4a49-4a4d-bdad-77ef345b9660",
   "metadata": {},
   "source": [
    "# Text preprocessing: Pytorch, NLTK and SKLEARN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9d4bca-4c33-4a2f-88bd-60505aae9431",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Extracting tokens(words) from the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0f1247c-8a0e-43cb-b486-7324bf46fc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0dec61ac-67b8-4be2-8e7f-4d7911e5f207",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer('basic_english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38388fe5-9946-461d-b697-2e60b48531f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"\"\"\n",
    "A large language model (LLM) is a language model notable for its ability \n",
    "to achieve general-purpose language generation and other natural language processing tasks such as classification. \n",
    "LLMs acquire these abilities by learning statistical relationships from text documents during a computationally \n",
    "intensive self-supervised and semi-supervised training process.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ac2d572-fa17-417e-872d-44f3d3ad47cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18bf1f6a-dbac-4930-b439-aeeea60bb3ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'large', 'language', 'model', '(', 'llm', ')', 'is', 'a', 'language', 'model', 'notable', 'for', 'its', 'ability', 'to', 'achieve', 'general-purpose', 'language', 'generation', 'and', 'other', 'natural', 'language', 'processing', 'tasks', 'such', 'as', 'classification', '.', 'llms', 'acquire', 'these', 'abilities', 'by', 'learning', 'statistical', 'relationships', 'from', 'text', 'documents', 'during', 'a', 'computationally', 'intensive', 'self-supervised', 'and', 'semi-supervised', 'training', 'process', '.']\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2dee35-87f7-43a1-bb8f-b44dd4ce9191",
   "metadata": {},
   "source": [
    "## Stop word removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300962f0-229b-48d6-b7cf-a4f9b51b8002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b3cb64e-ecbc-47ef-be10-eda26b8da20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40a83fda-c852-4902-9837-c7f66db31cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3726d989-0c3a-4206-82fb-0f3c6e25db7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = list(set(tokens) - set(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84c7898f-21ed-4104-899a-bcbc6249e01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abilities', 'large', 'generation', '(', '.', 'classification', 'process', 'acquire', 'language', 'training', 'statistical', 'self-supervised', 'relationships', 'tasks', 'model', 'learning', 'natural', 'intensive', 'text', ')', 'semi-supervised', 'ability', 'achieve', 'notable', 'llms', 'llm', 'computationally', 'documents', 'general-purpose', 'processing']\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c89843-5a6e-4809-b9f1-9f59070fabf9",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91e4a894-8b92-4982-bf03-5bc762d6280e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a298406-f5b6-4b10-87f7-602bba1f9ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b761e5b6-f7b5-458f-b1e3-4008ffd9840d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [stemmer.stem(i) for i in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a4679eb-8f3b-421a-ac56-11699dd88969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abil', 'larg', 'gener', '(', '.', 'classif', 'process', 'acquir', 'languag', 'train', 'statist', 'self-supervis', 'relationship', 'task', 'model', 'learn', 'natur', 'intens', 'text', ')', 'semi-supervis', 'abil', 'achiev', 'notabl', 'llm', 'llm', 'comput', 'document', 'general-purpos', 'process']\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e471f70b-95dd-4713-97c6-618b4e25e2d0",
   "metadata": {},
   "source": [
    "## Rare word removal\n",
    "\n",
    "Removing infrequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16f0f789-1d34-4ff8-bc64-855f87836b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "726bbbbc-139c-4cb2-8a73-e79b29a99d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_freq = FreqDist(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "307e179a-8c63-4882-82d0-5ab0af8cac27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'abil': 2, 'process': 2, 'llm': 2, 'larg': 1, 'gener': 1, '(': 1, '.': 1, 'classif': 1, 'acquir': 1, 'languag': 1, ...})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "13d401da-5336-4827-9673-776e7781d96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0c53b2d-3671-4296-8288-8004e699b94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [i for i in tokens if tokens_freq[i]>=threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5366b637-28ba-4bde-ae42-e20d80d29d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abil', 'process', 'abil', 'llm', 'llm', 'process']\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f51ff52-3c4e-4657-a592-b1d35d75faf7",
   "metadata": {},
   "source": [
    "## Encoding\n",
    "\n",
    "* One-hot encoding: words mapped into a binary vector\n",
    "* Bag of words (BOW): word frequency disregarding the order\n",
    "* TF-IDF: word uniquence and importance\n",
    "* Embeddings: convert words into vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ba156b-eebf-4060-baab-6630c546846a",
   "metadata": {},
   "source": [
    "### One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76d7f6e1-d555-4a49-8467-f291f52f891e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7a9b5ba3-1130-480b-8a6c-fa1518f7e6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = ['achieve', 'natural', 'ability']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cccc5f4b-2028-46b4-a2f4-476d7fc32b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_vectors = torch.eye(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "acf8b7d4-ba76-4324-8fbc-532ccb8a25b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_dict = {word: one_hot_vectors[i] for i, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5a34a093-3e74-4387-b36c-7fecfb0aa0fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'achieve': tensor([1., 0., 0.]),\n",
       " 'natural': tensor([0., 1., 0.]),\n",
       " 'ability': tensor([0., 0., 1.])}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a472580-6c5c-46ce-9f39-2de61d56dbf1",
   "metadata": {},
   "source": [
    "### BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cf10e5fb-57d9-49b4-947f-a051412b2e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b267424f-b17a-4c15-b9e3-6052e4ebdfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus.split('.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "856e087f-6f80-4277-84f9-4a33b7745f96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 41)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eea16a21-41d9-4046-95e4-1ffee69cc1cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 4, 1, 0, 1,\n",
       "        0, 2, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9ef2c43d-47b0-4425-bba1-83d810fdff5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abilities' 'ability' 'achieve' 'acquire' 'and' 'as' 'by'\n",
      " 'classification' 'computationally' 'documents' 'during' 'for' 'from'\n",
      " 'general' 'generation' 'intensive' 'is' 'its' 'language' 'large'\n",
      " 'learning' 'llm' 'llms' 'model' 'natural' 'notable' 'other' 'process'\n",
      " 'processing' 'purpose' 'relationships' 'self' 'semi' 'statistical' 'such'\n",
      " 'supervised' 'tasks' 'text' 'these' 'to' 'training']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f048257-d2f7-499e-ad15-0c52d5673134",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "73969f07-7f54-4325-b2dd-a1cc4a65d6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d0df67c7-5b80-4696-96e7-ea8a51335ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus.split('.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9e3c7231-c335-489f-90d0-c57f7887315f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 41)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape (sentence, word)\n",
    "X.toarray().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "14ec7965-fa24-47f5-88c3-f8af1ffe1f9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.15895379, 0.15895379, 0.        , 0.12088845,\n",
       "        0.15895379, 0.        , 0.15895379, 0.        , 0.        ,\n",
       "        0.        , 0.15895379, 0.        , 0.15895379, 0.15895379,\n",
       "        0.        , 0.15895379, 0.15895379, 0.63581516, 0.15895379,\n",
       "        0.        , 0.15895379, 0.        , 0.31790758, 0.15895379,\n",
       "        0.15895379, 0.15895379, 0.        , 0.15895379, 0.15895379,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.15895379,\n",
       "        0.        , 0.15895379, 0.        , 0.        , 0.15895379,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "83ea187c-5094-4290-b405-e7721b66e8d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abilities' 'ability' 'achieve' 'acquire' 'and' 'as' 'by'\n",
      " 'classification' 'computationally' 'documents' 'during' 'for' 'from'\n",
      " 'general' 'generation' 'intensive' 'is' 'its' 'language' 'large'\n",
      " 'learning' 'llm' 'llms' 'model' 'natural' 'notable' 'other' 'process'\n",
      " 'processing' 'purpose' 'relationships' 'self' 'semi' 'statistical' 'such'\n",
      " 'supervised' 'tasks' 'text' 'these' 'to' 'training']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6b239c-a7b1-4e56-bb4f-bf11042bbfc9",
   "metadata": {},
   "source": [
    "# Pytorch Dataset and Dataloader\n",
    "\n",
    "* Dataset is a container for preprocessed text\n",
    "* DataLoader is a batch loader with shuffle and multiprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f3884cd1-3964-4942-9cce-e10751bc9f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ac38c073-03ca-4ea8-9fe8-b7509f141d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, text):\n",
    "        self.text = text\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.text[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68867141-0920-41c8-a8b0-7bd2e88a430a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset(encoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52761d16-462b-4c0d-b53f-320d5248367c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f460e99a-4db4-40b6-ab44-43c9ba68a52c",
   "metadata": {},
   "source": [
    "# Example Shakespeare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "775bb4b9-3288-4d1d-854b-33b28a672e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "97dfe36a-00b4-4350-9c49-954612869318",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = io.open('./data/shakespeare.txt', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ad40eb97-7d24-4c92-87a4-fb032ea48618",
   "metadata": {},
   "outputs": [],
   "source": [
    "with text as file:\n",
    "    lines = [line.rstrip() for line in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3f8f3133-8e1e-4dbb-87a6-96c970b71f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = [line.lower() for line in lines if line!='']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bd860640-3d2b-4ad2-b02a-0c17dce61c13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2158"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "30f2efd6-433a-4b37-bfd4-513b0e22de4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the sonnets',\n",
       " 'by william shakespeare',\n",
       " 'from fairest creatures we desire increase,',\n",
       " \"that thereby beauty's rose might never die,\",\n",
       " 'but as the riper should by time decease,']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "780251fd-4ccf-453d-92bf-02f1cd4a0d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sonnet', 'william shakespear', 'fairest creatur desir increas ,', \"therebi beauti ' rose might never die ,\", 'riper time deceas ,']\n"
     ]
    }
   ],
   "source": [
    "# Create a list of stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Initialize the tokenizer and stemmer\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "stemmer = PorterStemmer() \n",
    "\n",
    "# Complete the function to preprocess sentences\n",
    "def preprocess_sentences(sentences):\n",
    "    processed_sentences = []\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.lower()\n",
    "        tokens = tokenizer(sentence)\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "        processed_sentences.append(' '.join(tokens))\n",
    "    return processed_sentences\n",
    "\n",
    "processed_shakespeare = preprocess_sentences(lines)\n",
    "print(processed_shakespeare[:5]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "392f6c17-8641-4f60-bf77-3c60f617f46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShakespeareDataset(Dataset):\n",
    "    def __init__(self, text):\n",
    "        self.text = text\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.text[idx]\n",
    "\n",
    "def encode_sentences(sentences):\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(sentences)\n",
    "    return X.toarray(), vectorizer\n",
    "\n",
    "def text_processing_pipeline(sentences):\n",
    "    processed_sentences = preprocess_sentences(sentences)\n",
    "    encoded_sentences, vectorizer = encode_sentences(processed_sentences)\n",
    "    dataset = ShakespeareDataset(encoded_sentences)\n",
    "    dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "    return dataloader, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0a0398e2-43f7-4a28-89c0-737c98f8d11e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ab' 'abhor' 'abid' 'abl' 'absenc']\n",
      "tensor([0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "dataloader, vectorizer = text_processing_pipeline(processed_shakespeare)\n",
    "\n",
    "# Print the vectorizer's feature names and the first 5 components of the first item\n",
    "print(vectorizer.get_feature_names_out()[:5]) \n",
    "print(next(iter(dataloader))[0, :5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
