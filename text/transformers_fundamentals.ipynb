{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47e865fc-2c54-4925-9281-bed60f0cae88",
   "metadata": {},
   "source": [
    "# Text processing with Transformers\n",
    "\n",
    "\n",
    "A transformer is a deep learning architecture for processing, understanding, and generating text in human language.\n",
    "\n",
    "It was developed by Google and based on the multi-head attention mechanism, proposed in a 2017 paper [\"Attention Is All You Need\"](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf).\n",
    "\n",
    "Some of the most impactful LLMs, including BERT, GPT, and T5, to name a few, are all based on transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b750c44-b8d9-4396-a8dd-baf3d0b739bb",
   "metadata": {},
   "source": [
    "## Transformer architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767d757d-880d-419c-86ad-ddae9c602498",
   "metadata": {},
   "source": [
    "<img src=\"./img/transformers.png\" alt=\"transformers\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35e4092-6634-487a-948a-f2e039522c04",
   "metadata": {},
   "source": [
    "Transformer has two main stacks:\n",
    "\n",
    "* Encoder\n",
    "\n",
    "* Decoder\n",
    "\n",
    "Each stack has number of layers containing Multi-Head Attention and Feed-Forward layers. \n",
    "\n",
    "They don't have recurrent or convolutional layers.\n",
    "\n",
    "**Transformers vs RNN:**\n",
    "\n",
    "* Transformers do not rely on recurrent layers as part of their neural network components. \n",
    "\n",
    "* They can significantly outperform RNNs in capturing long-range dependencies in large text data sequences, thanks to the so-called attention mechanisms, which together with token positional encoding, are capable of weighting the relative importance of different words in a sentence when making inferences.\n",
    "\n",
    "* Thanks to attention mechanisms, transformers handle tokens simultaneously rather than sequentially, leading to faster model training and inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79050866-edb8-412e-a81f-8094ea7d6214",
   "metadata": {},
   "source": [
    "**Types of transformer architecture:**\n",
    "\n",
    "* Encoder-Decoder: translation, summarization (T5, BART)\n",
    "* Encoder only: text classification, extractive QA (BERT)\n",
    "* Decoder only: text generation, generative QA (GPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3ac1fc-9960-4c15-88f1-a87f5351a7c2",
   "metadata": {},
   "source": [
    "# PyTorch Transformer\n",
    "\n",
    "The model dimension **d_model** refers to the dimensionality of embeddings used throughout the entire model to represent inputs, outputs, and the intermediate information processed in between. \n",
    "\n",
    "Attention mechanisms typically have multiple heads that perform parallel computations, specializing in capturing different types of text dependencies. The number of heads, specified in **nhead**, is normally set as a divisor of the model dimension. \n",
    "\n",
    "The depth of the model largely depends on the number of encoder and decoder layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12459f89-27c6-4aaf-89ae-0bb004cd1a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64700253-8716-41e1-b35c-8dcd1a722f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 512\n",
    "nhead = 8\n",
    "num_encoder_layers = 6\n",
    "num_decoder_layers = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8eff1a-1b17-4e21-833d-34d5eb124500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Transfomer implementation\n",
    "model = nn.Transformer(\n",
    "    d_model=d_model,\n",
    "    nhead=n_heads,\n",
    "    num_encoder_layers=num_encoder_layers,\n",
    "    num_decoder_layers=num_decoder_layers,\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "913fd1c6-6640-4e51-b192-1bf7a90205d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (decoder): TransformerDecoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        (dropout3): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f295f68d-cdea-40e3-ab8a-a718ab076e2e",
   "metadata": {},
   "source": [
    "# Attention mechanism & Positional encoding\n",
    "\n",
    "<img src=\"./img/attention.png\" alt=\"attention.png\" style=\"width: 400px;\"/>\n",
    "\n",
    "The Attention Mechanism in the transformer assigns importance to words within a sentence. \n",
    "\n",
    "In example, 'it' is understood to be more related to 'animal', 'street' and 'the' in descending order of significance. This ensures that in tasks like translation, the machine's interpretation aligns with the human understanding.\n",
    "\n",
    "**SELF** and **MULTI-HEAD** attention:\n",
    "\n",
    "* Self-Attention assigns significance to words within a sentence. In *The cat, which was on the roof, was scared,* the mechanism links \"was scared\" directly to \"The cat\". \n",
    "\n",
    "* Multi-Head Attention is akin to deploying multiple spotlights. In the same example, \"was scared\" could relate to \"The cat,\" signify \"the roof,\" or point to \"was on\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb072f3-0e22-4959-a761-fe84ef1a050a",
   "metadata": {},
   "source": [
    "## Positional encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a104fc5d-10b5-4eb7-89fa-7e5627f06aab",
   "metadata": {},
   "source": [
    "Attention mechanism requires information about the *position of each token in the sequence*. \n",
    "\n",
    "The positional encoding precedes attention layer and supplies information about the position of each token in a sequence.\n",
    "\n",
    "Instead of token index, Transformers use a positional encoding scheme, where each position/index is mapped to a vector calculated by sine and cosine functions of varying frequencies.\n",
    "\n",
    "<img src=\"./img/positional_encoding.png\" alt=\"positional_encoding\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d39729-2184-4d59-91fe-9d0e51709e2c",
   "metadata": {},
   "source": [
    "The output of the positional encoding layer is a matrix, where each row of the matrix represents an encoded object of the sequence summed with its positional information.\n",
    "\n",
    "<img src=\"./img/positional_encoding_end.png\" alt=\"positional_encoding_end\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8c591f9-465d-4166-ab6d-9926b7fdc301",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length=512):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "        # initialize positional encoding matrix\n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "\n",
    "        # initialize position of indices in the sequence\n",
    "        # 'unsqueeze' function aligns the tensor shape with the shape of the input embeddings\n",
    "        position = torch.arange(0, max_seq_length, dtype=float).unsqueeze(1)\n",
    "\n",
    "        # scaler for positional indices\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2, dtype=float) *\n",
    "            -(math.log(10000.0) / d_model)\n",
    "        )\n",
    "        # apply scaler to positional indices combined with sine and cosine functions\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # unsqueeze to add batch dimension\n",
    "        pe = pe.unsqueeze(0)\n",
    "\n",
    "        # set matrix as non-trainable using register_buffer\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # add the positional encodings to the whole sequence embeddings contained in tensor x\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09d6c6e-200b-46da-b9cc-066facf95020",
   "metadata": {},
   "source": [
    "## Attention mechanism in details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e72aec-fdd3-472d-b7f7-bed55d8b164d",
   "metadata": {},
   "source": [
    "<img src=\"./img/attention_mechanism.png\" alt=\"attention_mechanism\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6358556f-c7c6-4ac3-9eb5-eda1473708ed",
   "metadata": {},
   "source": [
    "Each embedding is first projected into three matrices of equal dimension -query, key, and values- by applying three separate linear transformations each having learned their own weights during training.\n",
    "\n",
    "Scaled dot-product is the most common self-attention approach, which applies dot-product (or cosine) similarity between every query-key pair in a sequence to yield a matrix of attention scores between words.\n",
    "\n",
    "Softmax scaling helps obtain a matrix of attention weights, indicating the relevance or attention that the model must pay to each token in a sequence like \"orange is my favorite fruit\" for a given query token, such as \"orange\". In this example, \"favorite\" and \"fruit\" are the two words to pay the highest attention to when processing the word \"orange\".\n",
    "\n",
    "Attention weights are then multiplied by the values to obtain updated token embeddings with relevant information about the sequence.\n",
    "\n",
    "Transformers implemeted with multiple attention heads to learn various tasks, see on the picture below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c19f5a8-0120-4db8-ad92-8d4ac3baa975",
   "metadata": {},
   "source": [
    "<img src=\"./img/multi_headed_attention.png\" alt=\"multi_headed_attention\" style=\"width: 800px;\"/>\n",
    "\n",
    "Multi-headed attention concatenates attention-head outputs and linearly projects them to keep consistent embedding dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7ef5c6c-d63b-4ec8-8a84-8cd70b078aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        # number of attention heads handling embedding size head_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "\n",
    "        # linear transformations for attention inputs\n",
    "        self.query_linear = nn.Linear(d_model, d_model)\n",
    "        self.key_linear = nn.Linear(d_model, d_model)\n",
    "        self.value_linear = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # final concatenated output\n",
    "        self.output_layer = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        # splits the input across the heads\n",
    "        x = x.view(batch_size, -1, self.num_heads, self.head_dim)\n",
    "        return x.permute(0,2,1,3).contiguous().view(batch_size * self.num_heads, -1 , self.head_dim)\n",
    "\n",
    "    def compute_attention(self, query, key, mask=None):\n",
    "        # calculates the attention weights inside each head\n",
    "        scores = torch.matmul(query, key.permute(1,2,0))\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float(\"-1e9\"))\n",
    "        attention_weights = torch.functional.softmax(scores, dim=-1)\n",
    "        return attention_weights\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        query = self.split_heads(\n",
    "            self.query_linear(query), batch_size\n",
    "        )\n",
    "        key = self.split_heads(\n",
    "            self.key_linear(key), batch_size\n",
    "        )\n",
    "        value = self.split_heads(\n",
    "            self.value_linear(value), batch_size\n",
    "        )\n",
    "\n",
    "        attention_weights = self.compute_attention(query, key, mask)\n",
    "\n",
    "        output = torch.matmul(attention_weights, value)\n",
    "        output = output.view(\n",
    "            batch_size, \n",
    "            self.num_heads, \n",
    "            -1, \n",
    "            self.head_dim\n",
    "        ).permute(0,2,1,3).contiguous().view(batch_size, -1 , self.d_model)\n",
    "\n",
    "        return self.output_layer(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07804174-2b6c-4e98-91e3-c4c1f3350d1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
