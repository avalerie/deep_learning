{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fadabc89-f445-42cd-a6f1-f6ce1648b569",
   "metadata": {},
   "source": [
    "# Transformers Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3dd17a-183f-4c14-856f-40d621651d45",
   "metadata": {},
   "source": [
    "It is designed to handle autoregressive sequence generation tasks like text generation and completion.\n",
    "\n",
    "The architecture is similar to encoder-only transformer with two differences:\n",
    "\n",
    "* **Masked multi-head self-attention**. It helps the model specialize in predicting the next word in a sequence one step at a time, iteratively generating messages, answers, or any text just like GPT and other autoregressive LLMs do. For each token in the target sequence, only the previously generated tokens are observed, whereas subsequent tokens are hidden by using an upper triangular mask that prevents attending to future positions.\n",
    "\n",
    "* **Transformer Head** model consists of a linear layer with softmax activation over the entire vocabulary to estimate the likelihood of each word or token being the next one to generate, and returning the most likely one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9ff41c-2001-4943-b883-2b9b13a737a8",
   "metadata": {},
   "source": [
    "## Masked Self-Attention\n",
    "\n",
    "<img src=\"./img/masked_self_attn.png\" alt=\"masked_self_attn\" style=\"width: 300px;\"/>\n",
    "\n",
    "Mask is a triangular matrix. By passing this matrix to the attention heads, each token in the sequence only pays attention to \"past\" information on its left-hand side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f75ba52-4a46-4320-be14-4ad1f0a3abc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "453a2ca3-e9b4-4daf-a9e4-7a470100b3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8832700-5c56-4563-a254-689b89c6735e",
   "metadata": {},
   "outputs": [],
   "source": [
    "self_attention_mask = (\n",
    "    1 - torch.triu(\n",
    "        torch.ones(1, sequence_length, sequence_length), diagonal=1\n",
    "    )\n",
    ").bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0863937-b284-4877-a920-66dbd204102a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ True, False, False, False, False],\n",
       "         [ True,  True, False, False, False],\n",
       "         [ True,  True,  True, False, False],\n",
       "         [ True,  True,  True,  True, False],\n",
       "         [ True,  True,  True,  True,  True]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self_attention_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88015cad-9f83-417c-b6af-50024934b77f",
   "metadata": {},
   "source": [
    "## Transformer Decoder from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0b652ce-7936-4a4e-875f-7c20659e7424",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers_utils import PositionalEncoder, MultiHeadAttention, FeedForwardTransformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4750f48a-8f92-49c1-848b-7d01e3dbfeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = FeedForwardTransformation(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        attn_output = self.attention(x,x,x,mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_seq_length):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoder(d_model, max_seq_length)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)]\n",
    "        )\n",
    "        # linear layer (head) for next-word prediction\n",
    "        self.fc = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x, self_mask):\n",
    "        x = self.embedding(x)\n",
    "        x = self.poistional_encoding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, self_mask)\n",
    "        # Apply the forward pass through the model head\n",
    "        x = self.fc(x)\n",
    "        return nn.functional.log_softmax(x, dim=-1)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5564206-a5fe-49bf-a49f-a597365eee36",
   "metadata": {},
   "source": [
    "## Testing Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7e9b69b-deb6-4254-accc-a2922060c822",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 3\n",
    "vocab_size = 10000\n",
    "batch_size = 8\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "d_ff = 2048\n",
    "sequence_length = 64\n",
    "dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2427ec64-e035-4b3b-b62c-4502eec628fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequence = torch.randint(0, vocab_size, (batch_size, sequence_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a552902-aefa-46ed-a829-e6ea0ab7ff66",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Triangular attention mask for causal attention\n",
    "self_attention_mask = (\n",
    "    1 - torch.triu(\n",
    "        torch.ones(1, sequence_length, sequence_length), diagonal=1\n",
    "    )\n",
    ").bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83d7b520-91e3-4536-bf22-2e519146452c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 64, 10000])\n",
      "tensor([[ -8.8693,  -9.6007,  -8.9304,  ...,  -9.6856,  -9.3465,  -9.6564],\n",
      "        [ -9.8812, -10.1586, -10.0967,  ...,  -8.9913,  -9.0750,  -9.9167],\n",
      "        [ -8.8258,  -9.0934,  -9.8143,  ...,  -9.5052,  -9.8370,  -8.6690],\n",
      "        ...,\n",
      "        [ -9.6214,  -9.4889,  -8.9430,  ...,  -9.5053,  -9.2300,  -9.4115],\n",
      "        [ -8.7715, -10.4404,  -8.6053,  ...,  -9.7100,  -8.7736,  -8.5819],\n",
      "        [ -7.7874,  -9.9393,  -9.2000,  ...,  -9.2514,  -8.7640,  -9.0817]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the decoder transformer\n",
    "decoder = TransformerDecoder(vocab_size, d_model, num_layers, num_heads, d_ff, dropout, sequence_length)\n",
    "\n",
    "output = decoder(input_sequence, self_attention_mask)\n",
    "print(output.shape)\n",
    "print(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517234b8-f29e-4b32-adce-0f6d0ed94cc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
