{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99b5d59f",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "## Text classification with sequential models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b78197",
   "metadata": {},
   "source": [
    "Recurrent Neural Networks for text:\n",
    "\n",
    "* Handle sequences of varying lengths \n",
    "\n",
    "* Maintain an internal short-term memory, enabling them to learn patterns across time\n",
    "\n",
    "* CNNs spot patterns in chunks of text, while RNNs remember past words to understand the whole sentence's meaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35de89ad",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffeedab9-8b63-4870-96df-268ea1dd2429",
   "metadata": {},
   "source": [
    "[Movies sentiment analysis dataset](https://www.kaggle.com/datasets/yshiml/movie-datacsv?resource=download&select=movie_data.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ee82928-259d-4b3c-bbc4-22fe95d9d152",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./data/movie_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55ab09d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "10b35ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalysisDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Return features and label for single sample at index idx\n",
    "        \"\"\"\n",
    "        text = self.data[idx,:-1]\n",
    "        label = self.data[idx,-1]\n",
    "        return text, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee1f6733-027d-4b5e-88f5-5e72dea78a62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review       Read the book, forget the movie!\n",
       "sentiment                                   0\n",
       "Name: 44585, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[44585]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3c0e69-9282-469c-89ff-9e9d3cb5c6f5",
   "metadata": {},
   "source": [
    "## Preprocess text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4c40bda4-1d98-4a1f-a614-38ec03c3f167",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "01439c59-7d87-477e-b4f9-047fafdc72a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Initialize the tokenizer and stemmer\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "stemmer = PorterStemmer() \n",
    "\n",
    "# Complete the function to preprocess sentences\n",
    "def preprocess_sentences(sentences):\n",
    "    processed_sentences = []\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.lower()\n",
    "        tokens = tokenizer(sentence)\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "        processed_sentences.append(' '.join(tokens))\n",
    "    return processed_sentences\n",
    "\n",
    "def encode_sentences(sentences):\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(sentences)\n",
    "    return X.toarray(), vectorizer\n",
    "\n",
    "def text_processing_pipeline(sentences):\n",
    "    processed_sentences = preprocess_sentences(sentences)\n",
    "    encoded_sentences, vectorizer = encode_sentences(processed_sentences)\n",
    "    dataset = SentimentAnalysisDataset(encoded_sentences)\n",
    "    dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "    return dataloader, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f5a8c34d-4182-431e-bdf1-9e0b76cb3ec6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Read the book, forget the movie!'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[44585].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c8780cc2-e368-4c51-a171-33ecda235d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader, vectorizer = text_processing_pipeline(df.review[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95458f12-ac8e-48ea-a76d-bbd7857a3982",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "988e14b5-4558-45fc-959c-9a559e4f2a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf2919f-1ebf-47f7-8939-7e43561d0198",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super.__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        out = out[:, -1, :] \n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "        \n",
    "# Initialize the model\n",
    "rnn_model = RNNModel(input_size, hidden_size, num_layers, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(rnn_model.parameters(), lr=0.01)\n",
    "\n",
    "# Train the model for ten epochs and zero the gradients\n",
    "for epoch in range(10): \n",
    "    optimizer.zero_grad()\n",
    "    outputs = rnn_model(X_train_seq)\n",
    "    loss = criterion(outputs, y_train_seq)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Epoch: {epoch+1}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afff90c-6901-48f7-b0fb-20243ee2b6d4",
   "metadata": {},
   "source": [
    "## LSTM\n",
    "\n",
    "LSTM architecture: input, forget and output gates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dd15c240-f33a-4c50-8a98-4e1ebccb484e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super.__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)        \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = out[:, -1, :] \n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# Initialize model with required parameters\n",
    "lstm_model = LSTMModel(input_size, hidden_size, num_layers, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(lstm_model.parameters(), lr=0.01)\n",
    "\n",
    "# Train the model by passing the correct parameters and zeroing the gradient\n",
    "for epoch in range(10): \n",
    "    optimizer.zero_grad()\n",
    "    outputs = lstm_model(X_train_seq)\n",
    "    loss = criterion(outputs, y_train_seq)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Epoch: {epoch+1}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73b86a2-5570-477a-a09b-23a2e37bbb00",
   "metadata": {},
   "source": [
    "## GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "288bfdbe-7f34-4300-b61a-1c79740f02c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)       \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size) \n",
    "        out, _ = self.gru(x, h0)\n",
    "        out = out[:, -1, :] \n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# Initialize the model\n",
    "gru_model = GRUModel(input_size, hidden_size, num_layers, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(gru_model.parameters(), lr=0.01)\n",
    "\n",
    "# Train the model and backpropagate the loss after initialization\n",
    "for epoch in range(15): \n",
    "    optimizer.zero_grad()\n",
    "    outputs = gru_model(X_train_seq)\n",
    "    loss = criterion(outputs, y_train_seq)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Epoch: {epoch+1}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb551948-9c62-48ff-a8b5-b94075b79ab7",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6992f2ab-2288-4bd9-b7e3-c11f344df9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "25794399-dda9-42bc-9839-fe18edb6a227",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_function = Accuracy(task='binary', num_classes=2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e33106-75f9-4601-932b-e02da1be139c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation loop\n",
    "for epoch in range(10):\n",
    "    outputs = rnn_model(X_train)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    eval_function(predicted, y_test_seq)\n",
    "    \n",
    "eval_function.compute()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
