{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47e865fc-2c54-4925-9281-bed60f0cae88",
   "metadata": {},
   "source": [
    "# Text processing with Transformers\n",
    "\n",
    "\n",
    "A transformer is a deep learning architecture developed by Google and based on the multi-head attention mechanism, proposed in a 2017 paper [\"Attention Is All You Need\"](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12459f89-27c6-4aaf-89ae-0bb004cd1a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99bfb2fb-1f27-4efa-a7b6-1258e7044371",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_texts = [\n",
    "    'I love this product',\n",
    "    'This is terrible',\n",
    "    'Could be better',\n",
    "    'This is the best',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5e6590d-2bec-4be3-a9c1-40ea5634f8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [1,0,0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90440e89-add2-47dc-a1b0-dadf924cf03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = sample_texts[:3], sample_texts[3:]\n",
    "train_labels, test_labels = labels[:3], labels[3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b542a5-56e0-406f-bbc0-f0572c593954",
   "metadata": {},
   "source": [
    "TransformerEncoderLayer:\n",
    "\n",
    "* d_model - influences the model's representational depth\n",
    "* nhead - determines how many word contexts the model can focus on simultaneously, impacting its contextual understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28eba0bf-75e6-44ac-8cd9-e199efeed347",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, embed_size, heads, num_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=embed_size, nhead=heads),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        self.fc = nn.Linear(embed_size, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = x.mean(dim=1)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02b826f-6442-44f8-a953-5d8be8687825",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerEncoder(embed_size=512, heads=8, num_layers=3, dropout=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54074482-8bac-4b9c-8fd4-dd89b6810eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ff319e-a18a-4b16-89ca-270f651766c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "for epoch in range(10):\n",
    "    for sentence, label in zip(train_data,train_labels):\n",
    "        # Split the sentences into tokens and stack the embeddings\n",
    "        tokens = sentence.split()\n",
    "        data = torch.stack([token_embeddings[i] for i in tokens], dim=1)\n",
    "        output = model(data)\n",
    "        loss = loss_function(output, torch.tensor([label]))\n",
    "        # Zero the gradients and perform a backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backwards()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f880f195-54b2-4092-ac3c-19baf57433ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "def predict(sentence):\n",
    "    model.eval()\n",
    "    # Deactivate the gradient computations and get the sentiment prediction.\n",
    "    with torch.no_grad():\n",
    "        tokens = sentence.split()\n",
    "        data = torch.stack([token_embeddings.get(token, torch.rand((1, 512))) for i in tokens], dim=1)\n",
    "        output = model(data)\n",
    "        predicted = torch.argmax(output, dim=1)\n",
    "    return 'positive' if predicted.item() == 1 else 'negative'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f295f68d-cdea-40e3-ab8a-a718ab076e2e",
   "metadata": {},
   "source": [
    "# Attention mechanism\n",
    "\n",
    "<img src=\"./img/attention.png\" alt=\"attention.png\" style=\"width: 400px;\"/>\n",
    "\n",
    "The Attention Mechanism in the transformer assigns importance to words within a sentence. \n",
    "\n",
    "In example, 'it' is understood to be more related to 'animal', 'street' and 'the' in descending order of significance. This ensures that in tasks like translation, the machine's interpretation aligns with the human understanding.\n",
    "\n",
    "**SELF** and **MULTI-HEAD** attention:\n",
    "\n",
    "* Self-Attention assigns significance to words within a sentence. In *The cat, which was on the roof, was scared,* the mechanism links \"was scared\" directly to \"The cat\". \n",
    "\n",
    "* Multi-Head Attention is akin to deploying multiple spotlights. In the same example, \"was scared\" could relate to \"The cat,\" signify \"the roof,\" or point to \"was on\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026bc9d0-7132-4231-98b0-f7bfabe110f7",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba1504d0-d804-44fa-b17d-dc3e57b7cdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = [\n",
    "    \"the animal didn't cross the street because it was too tired\",\n",
    "    \"the cat sat on the mat\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73f23736-2adb-4ca5-9054-e6ef06ae691a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary and word index\n",
    "vocab = set(' '.join(sample_text).split())\n",
    "word_to_idx = {word:idx for idx, word in enumerate(vocab)}\n",
    "ix_to_word = {idx:word for idx, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e83fc39-edff-411c-8a72-cc422042ba13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder/decoder data\n",
    "pairs = [sentence.split() for sentence in sample_text]\n",
    "input_data = [[word_to_idx[word] for word in sentence[:-1]] for sentence in pairs]\n",
    "target_data = [word_to_idx[sentence[-1]] for sentence in pairs]\n",
    "\n",
    "inputs = [torch.tensor(seq, dtype=torch.long) for seq in input_data]\n",
    "targets = torch.tensor(target_data, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e3bb80-9d9c-499b-8322-241201548051",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7ab8e298-6ced-46e9-af0b-75ecba0695ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 10\n",
    "hidden_dim = 16\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "class RNNWithAttentionModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embedding layer translates word indexes to vectors\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # RNN layer for sequentail processing\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "\n",
    "        # Attention layer computes word significanes, performing linear transformation of hidden_dim to one,\n",
    "        # yielding a singular attention score per word\n",
    "        self.attention = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "        # Final layer outputting vocab_size pinpoints the predicted word index\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # word indexes are embedded\n",
    "        x = self.embeddings(x)\n",
    "\n",
    "        # process embeddings in sequentail layer generating output for each word\n",
    "        out, _ = self.rnn(x)\n",
    "\n",
    "        # attention scores are derived by applying a linear transformation to the RNN outputs, \n",
    "        # normalizing using softmax, and reshaping the tensor using squeeze two to simplify attention calculations.\n",
    "        att_weights = torch.nn.functional.softmax(self.attention(out).squeeze(2), dim=1)\n",
    "\n",
    "        # Context vector is formulated by multiplying attention scores with RNN outputs, \n",
    "        # creating a weighted sum of the outputs, where weights are the attention scores. \n",
    "        # The unsqueeze two operation is important for adjusting tensor dimensions for matrix multiplication with RNN outputs. \n",
    "        # The context vector is then summed using torch-dot-sum to feed into the fc layer for the final prediction.\n",
    "        context = torch.sum(att_weights.unsqueeze(2) * out, dim=1)\n",
    "        \n",
    "        return self.fc(context)\n",
    "\n",
    "def pad_sequences(batch):\n",
    "    \"\"\"\n",
    "    Ensures consistent sequence lengths by padding the input sequences \n",
    "    with torch-dot-cat and torch-dot-stack, avoiding any potential length discrepancies\n",
    "    \"\"\"\n",
    "    max_len = max([len(i) for i in batch])\n",
    "    return torch.stack(\n",
    "        [torch.cat([seq, torch.zeros(max_len - len(seq)).long()])\n",
    "             for seq in batch]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6c62e071-abc0-4600-8188-ff1c3bbecc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_finction = nn.CrossEntropyLoss()\n",
    "model = RNNWithAttentionModel(vocab_size, embedding_dim, hidden_dim)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "db48fbf4-b771-4569-ba85-2dba4fd4233f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss: 0.0005866951541975141\n",
      "epoch 100, loss: 0.0004257845284882933\n",
      "epoch 200, loss: 0.00032556717633269727\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "for epoch in range(300):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    padded_inputs = pad_sequences(inputs)\n",
    "    output = model(padded_inputs)\n",
    "    loss = loss_finction(output, targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch % 100) == 0:\n",
    "        print(f'epoch {epoch}, loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "617b0be9-e43c-437d-883e-c61b58ea121a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input: the animal didn't cross the street because it was too\n",
      "\n",
      "Target: tired\n",
      "\n",
      "Output: tired\n",
      "\n",
      "Input: the cat sat on the\n",
      "\n",
      "Target: mat\n",
      "\n",
      "Output: mat\n"
     ]
    }
   ],
   "source": [
    "# testing\n",
    "for seq, target in zip(input_data, target_data):\n",
    "    input_test = torch.tensor(seq, dtype=torch.long).unsqueeze(0)\n",
    "    model.eval()\n",
    "    output = model(input_test)\n",
    "    predictions = ix_to_word[torch.argmax(output).item()]\n",
    "    print(f'\\nInput: {\" \".join([ix_to_word[i] for i in seq])}')\n",
    "    print(f'\\nTarget: {ix_to_word[target]}')\n",
    "    print(f'\\nOutput: {predictions}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5347ac-d26f-4a80-bd24-c33d5dc880ef",
   "metadata": {},
   "source": [
    "# Adversarial attacks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e767b7-7134-48ee-bc51-b86b141f01a5",
   "metadata": {},
   "source": [
    "What is an Adversarial Attack in AI? \n",
    "\n",
    "It is an attack where the goal is to cause an AI system to make a mistake or misclassification, often through subtle manipulations of the input data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e61ccb0-4d68-45b8-aa75-081c94fc7d14",
   "metadata": {},
   "source": [
    "**Fast Gradient Sign Method (FGSM)**\n",
    "\n",
    "<img src=\"./img/fgsm.png\" alt=\"fgsm.png\" style=\"width: 600px;\"/>\n",
    "\n",
    "It uses precise changes that may go undetected. By exploiting the learning information of a model, it can introduce the tiniest changes to the input, leading the model astray. \n",
    "\n",
    "Example of a spam filter that's usually accurate but gets deceived by a cleverly altered email. Notice the tiny tweak in the word \"love\". To an AI model, this could change the classification. In our real-world example, such alterations can prevent a spammy email from being flagged."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15dd93d-997a-4e63-962f-08953b15f67b",
   "metadata": {},
   "source": [
    "**Projected Gradient Descent (PGD)** \n",
    "\n",
    "<img src=\"./img/pgd.png\" alt=\"pgd\" style=\"width: 600px;\"/>\n",
    "\n",
    "It's like the seasoned burglar who picks the lock step by step. \n",
    "\n",
    "It refines its deception across several iterations, ensuring the most effective disturbance. Example of fake news detector, PGD could subtly adjust an article's phrasing over and over until the AI is convinced of its authenticity. Here, likely becomes set to, altering the prediction confidence. If this were a fake news detector, such iterative tweaks could confuse AI's judgment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e373847-2fa0-4c79-8ed3-14b96d06009f",
   "metadata": {},
   "source": [
    "**The Carlini & Wagner (C&W) attack**\n",
    "\n",
    "<img src=\"./img/cw.png\" alt=\"cw\" style=\"width: 600px;\"/>\n",
    "\n",
    "It's like the mastermind spy who leaves no trace. \n",
    "\n",
    "By focusing on optimizing a loss function, it ensures that the modifications are not just deceptive to the AI but virtually undetectable to us. Consider an AI-driven stock trading system; C&W could tweak a financial transcript subtly, potentially causing erroneous investments. The addition of \"somewhat\" can change the sentiment and context, especially if used in critical financial or medical reports."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22a24bf-ce1f-4e14-be8a-22859043cfe7",
   "metadata": {},
   "source": [
    "**Defence strategies:**\n",
    "\n",
    "* Model ensembling\n",
    "* Data Augmentation\n",
    "* Adversarial Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e60f5d-d663-43f9-8261-0dd70ede139f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
