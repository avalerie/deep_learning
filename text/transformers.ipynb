{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47e865fc-2c54-4925-9281-bed60f0cae88",
   "metadata": {},
   "source": [
    "# Text processing with Transformers\n",
    "\n",
    "\n",
    "A transformer is a deep learning architecture for processing, understanding, and generating text in human language.\n",
    "\n",
    "It was developed by Google and based on the multi-head attention mechanism, proposed in a 2017 paper [\"Attention Is All You Need\"](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf).\n",
    "\n",
    "Some of the most impactful LLMs, including BERT, GPT, and T5, to name a few, are all based on transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b750c44-b8d9-4396-a8dd-baf3d0b739bb",
   "metadata": {},
   "source": [
    "## Transformer architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767d757d-880d-419c-86ad-ddae9c602498",
   "metadata": {},
   "source": [
    "<img src=\"./img/transformers.png\" alt=\"transformers\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35e4092-6634-487a-948a-f2e039522c04",
   "metadata": {},
   "source": [
    "Transformer has two main stacks:\n",
    "\n",
    "* Encoder\n",
    "\n",
    "* Decoder\n",
    "\n",
    "Each stack has number of layers containing Multi-Head Attention and Feed-Forward layers. \n",
    "\n",
    "They don't have recurrent or convolutional layers.\n",
    "\n",
    "**Transformers vs RNN:**\n",
    "\n",
    "* Transformers do not rely on recurrent layers as part of their neural network components. \n",
    "\n",
    "* They can significantly outperform RNNs in capturing long-range dependencies in large text data sequences, thanks to the so-called attention mechanisms, which together with token positional encoding, are capable of weighting the relative importance of different words in a sentence when making inferences.\n",
    "\n",
    "* Thanks to attention mechanisms, transformers handle tokens simultaneously rather than sequentially, leading to faster model training and inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79050866-edb8-412e-a81f-8094ea7d6214",
   "metadata": {},
   "source": [
    "**Types of transformer architecture:**\n",
    "\n",
    "* Encoder-Decoder: translation, summarization (T5, BART)\n",
    "* Encoder only: text classification, extractive QA (BERT)\n",
    "* Decoder only: text generation, generative QA (GPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3ac1fc-9960-4c15-88f1-a87f5351a7c2",
   "metadata": {},
   "source": [
    "# PyTorch Transformer\n",
    "\n",
    "The model dimension **d_model** refers to the dimensionality of embeddings used throughout the entire model to represent inputs, outputs, and the intermediate information processed in between. \n",
    "\n",
    "Attention mechanisms typically have multiple heads that perform parallel computations, specializing in capturing different types of text dependencies. The number of heads, specified in **nhead**, is normally set as a divisor of the model dimension. \n",
    "\n",
    "The depth of the model largely depends on the number of encoder and decoder layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12459f89-27c6-4aaf-89ae-0bb004cd1a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64700253-8716-41e1-b35c-8dcd1a722f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 512\n",
    "nhead = 8\n",
    "num_encoder_layers = 6\n",
    "num_decoder_layers = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8eff1a-1b17-4e21-833d-34d5eb124500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Transfomer implementation\n",
    "model = nn.Transformer(\n",
    "    d_model=d_model,\n",
    "    nhead=n_heads,\n",
    "    num_encoder_layers=num_encoder_layers,\n",
    "    num_decoder_layers=num_decoder_layers,\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "913fd1c6-6640-4e51-b192-1bf7a90205d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (decoder): TransformerDecoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        (dropout3): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f295f68d-cdea-40e3-ab8a-a718ab076e2e",
   "metadata": {},
   "source": [
    "# Attention mechanism & Positional encoding\n",
    "\n",
    "<img src=\"./img/attention.png\" alt=\"attention.png\" style=\"width: 400px;\"/>\n",
    "\n",
    "The Attention Mechanism in the transformer assigns importance to words within a sentence. \n",
    "\n",
    "In example, 'it' is understood to be more related to 'animal', 'street' and 'the' in descending order of significance. This ensures that in tasks like translation, the machine's interpretation aligns with the human understanding.\n",
    "\n",
    "**SELF** and **MULTI-HEAD** attention:\n",
    "\n",
    "* Self-Attention assigns significance to words within a sentence. In *The cat, which was on the roof, was scared,* the mechanism links \"was scared\" directly to \"The cat\". \n",
    "\n",
    "* Multi-Head Attention is akin to deploying multiple spotlights. In the same example, \"was scared\" could relate to \"The cat,\" signify \"the roof,\" or point to \"was on\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb072f3-0e22-4959-a761-fe84ef1a050a",
   "metadata": {},
   "source": [
    "## Positional encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a104fc5d-10b5-4eb7-89fa-7e5627f06aab",
   "metadata": {},
   "source": [
    "Attention mechanism requires information about the *position of each token in the sequence*. \n",
    "\n",
    "The positional encoding precedes attention layer and supplies information about the position of each token in a sequence.\n",
    "\n",
    "Instead of token index, Transformers use a positional encoding scheme, where each position/index is mapped to a vector calculated by sine and cosine functions of varying frequencies.\n",
    "\n",
    "<img src=\"./img/positional_encoding.png\" alt=\"positional_encoding\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d39729-2184-4d59-91fe-9d0e51709e2c",
   "metadata": {},
   "source": [
    "The output of the positional encoding layer is a matrix, where each row of the matrix represents an encoded object of the sequence summed with its positional information.\n",
    "\n",
    "<img src=\"./img/positional_encoding_end.png\" alt=\"positional_encoding_end\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8c591f9-465d-4166-ab6d-9926b7fdc301",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class PositionalEncoder(nn.Module):\n",
    "    def __inti__(self, d_model, max_seq_length=512):\n",
    "        super().__init()\n",
    "        self.d_model = d_model\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "        # initialize positional encoding matrix\n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "\n",
    "        # initialize position of indices in the sequence\n",
    "        # 'unsqueeze' function aligns the tensor shape with the shape of the input embeddings\n",
    "        position = torch.arange(0, max_seq_length, dtype=float).unsqueeze(1)\n",
    "\n",
    "        # scaler for positional indices\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2, dtype=float) *\n",
    "            -(math.log(10000.0) / d_model)\n",
    "        )\n",
    "        # apply scaler to positional indices combined with sine and cosine functions\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # unsqueeze to add batch dimension\n",
    "        pe = pe.unsqueeze(0)\n",
    "\n",
    "        # set matrix as non-trainable using register_buffer\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # add the positional encodings to the whole sequence embeddings contained in tensor x\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09d6c6e-200b-46da-b9cc-066facf95020",
   "metadata": {},
   "source": [
    "## Attention mechanism in details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e72aec-fdd3-472d-b7f7-bed55d8b164d",
   "metadata": {},
   "source": [
    "<img src=\"./img/attention_mechanism.png\" alt=\"attention_mechanism\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6358556f-c7c6-4ac3-9eb5-eda1473708ed",
   "metadata": {},
   "source": [
    "Each embedding is first projected into three matrices of equal dimension -query, key, and values- by applying three separate linear transformations each having learned their own weights during training.\n",
    "\n",
    "Scaled dot-product is the most common self-attention approach, which applies dot-product (or cosine) similarity between every query-key pair in a sequence to yield a matrix of attention scores between words.\n",
    "\n",
    "Softmax scaling helps obtain a matrix of attention weights, indicating the relevance or attention that the model must pay to each token in a sequence like \"orange is my favorite fruit\" for a given query token, such as \"orange\". In this example, \"favorite\" and \"fruit\" are the two words to pay the highest attention to when processing the word \"orange\".\n",
    "\n",
    "Attention weights are then multiplied by the values to obtain updated token embeddings with relevant information about the sequence.\n",
    "\n",
    "Transformers implemeted with multiple attention heads to learn various tasks, see on the picture below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c19f5a8-0120-4db8-ad92-8d4ac3baa975",
   "metadata": {},
   "source": [
    "<img src=\"./img/multi_headed_attention.png\" alt=\"multi_headed_attention\" style=\"width: 800px;\"/>\n",
    "\n",
    "Multi-headed attention concatenates attention-head outputs and linearly projects them to keep consistent embedding dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7ef5c6c-d63b-4ec8-8a84-8cd70b078aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __inti__(self, d_model, num_heads):\n",
    "        super().__init()\n",
    "        self.d_model = d_model\n",
    "        # number of attention heads handling embedding size head_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "\n",
    "        # linear transformations for attention inputs\n",
    "        self.query_linear = nn.Linear(d_model, d_model)\n",
    "        self.key_linear = nn.Linear(d_model, d_model)\n",
    "        self.value_linear = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # final concatenated output\n",
    "        self.output_layer = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        # splits the input across the heads\n",
    "        x = x.view(batch_size, -1, self.num_heads, self.head_dim)\n",
    "        return x.permute(0,2,1,3).contiguous().view(batch_size * self.num_heads, -1 , self.head_dim)\n",
    "\n",
    "    def compute_attention(self, query, key, mask=None):\n",
    "        # calculates the attention weights inside each head\n",
    "        scores = torch.matmul(query, key.permute(1,2,0))\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float(\"-1e9\"))\n",
    "        attention_weights = nn.torch.functional.softmax(scores, dim=-1)\n",
    "        return attention_weights\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        query = self.split_heads(\n",
    "            self.query_linear(query), batch_size\n",
    "        )\n",
    "        key = self.split_heads(\n",
    "            self.key_linear(key), batch_size\n",
    "        )\n",
    "        value = self.split_heads(\n",
    "            self.value_linear(value), batch_size\n",
    "        )\n",
    "\n",
    "        attention_weights = self.compute_attention(query, key, mask)\n",
    "\n",
    "        output = torch.matmul(attention_weights, value)\n",
    "        output = output.view(\n",
    "            batch_size, \n",
    "            self.num_heads, \n",
    "            -1, \n",
    "            self.head_dim\n",
    "        ).permute(0,2,1,3).contiguous().view(batch_size, -1 , self.d_model)\n",
    "\n",
    "        return self.output_layer(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c77114-6375-4480-837e-d50afd75667d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d70dc045-38a1-4adc-b5d7-3d2a4dba2cb8",
   "metadata": {},
   "source": [
    "## TransformerEncoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99bfb2fb-1f27-4efa-a7b6-1258e7044371",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_texts = [\n",
    "    'I love this product',\n",
    "    'This is terrible',\n",
    "    'Could be better',\n",
    "    'This is the best',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5e6590d-2bec-4be3-a9c1-40ea5634f8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [1,0,0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90440e89-add2-47dc-a1b0-dadf924cf03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = sample_texts[:3], sample_texts[3:]\n",
    "train_labels, test_labels = labels[:3], labels[3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b542a5-56e0-406f-bbc0-f0572c593954",
   "metadata": {},
   "source": [
    "TransformerEncoderLayer:\n",
    "\n",
    "* d_model - influences the model's representational depth\n",
    "* nhead - determines how many word contexts the model can focus on simultaneously, impacting its contextual understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28eba0bf-75e6-44ac-8cd9-e199efeed347",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, embed_size, heads, num_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=embed_size, nhead=heads),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        self.fc = nn.Linear(embed_size, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = x.mean(dim=1)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02b826f-6442-44f8-a953-5d8be8687825",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerEncoder(embed_size=512, heads=8, num_layers=3, dropout=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54074482-8bac-4b9c-8fd4-dd89b6810eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ff319e-a18a-4b16-89ca-270f651766c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "for epoch in range(10):\n",
    "    for sentence, label in zip(train_data,train_labels):\n",
    "        # Split the sentences into tokens and stack the embeddings\n",
    "        tokens = sentence.split()\n",
    "        data = torch.stack([token_embeddings[i] for i in tokens], dim=1)\n",
    "        output = model(data)\n",
    "        loss = loss_function(output, torch.tensor([label]))\n",
    "        # Zero the gradients and perform a backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backwards()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f880f195-54b2-4092-ac3c-19baf57433ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "def predict(sentence):\n",
    "    model.eval()\n",
    "    # Deactivate the gradient computations and get the sentiment prediction.\n",
    "    with torch.no_grad():\n",
    "        tokens = sentence.split()\n",
    "        data = torch.stack([token_embeddings.get(token, torch.rand((1, 512))) for i in tokens], dim=1)\n",
    "        output = model(data)\n",
    "        predicted = torch.argmax(output, dim=1)\n",
    "    return 'positive' if predicted.item() == 1 else 'negative'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e3bb80-9d9c-499b-8322-241201548051",
   "metadata": {},
   "source": [
    "## RNN with Attention Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba1504d0-d804-44fa-b17d-dc3e57b7cdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = [\n",
    "    \"the animal didn't cross the street because it was too tired\",\n",
    "    \"the cat sat on the mat\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73f23736-2adb-4ca5-9054-e6ef06ae691a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary and word index\n",
    "vocab = set(' '.join(sample_text).split())\n",
    "word_to_idx = {word:idx for idx, word in enumerate(vocab)}\n",
    "ix_to_word = {idx:word for idx, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e83fc39-edff-411c-8a72-cc422042ba13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder/decoder data\n",
    "pairs = [sentence.split() for sentence in sample_text]\n",
    "input_data = [[word_to_idx[word] for word in sentence[:-1]] for sentence in pairs]\n",
    "target_data = [word_to_idx[sentence[-1]] for sentence in pairs]\n",
    "\n",
    "inputs = [torch.tensor(seq, dtype=torch.long) for seq in input_data]\n",
    "targets = torch.tensor(target_data, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7ab8e298-6ced-46e9-af0b-75ecba0695ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 10\n",
    "hidden_dim = 16\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "class RNNWithAttentionModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embedding layer translates word indexes to vectors\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # RNN layer for sequentail processing\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "\n",
    "        # Attention layer computes word significanes, performing linear transformation of hidden_dim to one,\n",
    "        # yielding a singular attention score per word\n",
    "        self.attention = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "        # Final layer outputting vocab_size pinpoints the predicted word index\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # word indexes are embedded\n",
    "        x = self.embeddings(x)\n",
    "\n",
    "        # process embeddings in sequentail layer generating output for each word\n",
    "        out, _ = self.rnn(x)\n",
    "\n",
    "        # attention scores are derived by applying a linear transformation to the RNN outputs, \n",
    "        # normalizing using softmax, and reshaping the tensor using squeeze two to simplify attention calculations.\n",
    "        att_weights = torch.nn.functional.softmax(self.attention(out).squeeze(2), dim=1)\n",
    "\n",
    "        # Context vector is formulated by multiplying attention scores with RNN outputs, \n",
    "        # creating a weighted sum of the outputs, where weights are the attention scores. \n",
    "        # The unsqueeze two operation is important for adjusting tensor dimensions for matrix multiplication with RNN outputs. \n",
    "        # The context vector is then summed using torch-dot-sum to feed into the fc layer for the final prediction.\n",
    "        context = torch.sum(att_weights.unsqueeze(2) * out, dim=1)\n",
    "        \n",
    "        return self.fc(context)\n",
    "\n",
    "def pad_sequences(batch):\n",
    "    \"\"\"\n",
    "    Ensures consistent sequence lengths by padding the input sequences \n",
    "    with torch-dot-cat and torch-dot-stack, avoiding any potential length discrepancies\n",
    "    \"\"\"\n",
    "    max_len = max([len(i) for i in batch])\n",
    "    return torch.stack(\n",
    "        [torch.cat([seq, torch.zeros(max_len - len(seq)).long()])\n",
    "             for seq in batch]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6c62e071-abc0-4600-8188-ff1c3bbecc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_finction = nn.CrossEntropyLoss()\n",
    "model = RNNWithAttentionModel(vocab_size, embedding_dim, hidden_dim)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "db48fbf4-b771-4569-ba85-2dba4fd4233f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss: 0.0005866951541975141\n",
      "epoch 100, loss: 0.0004257845284882933\n",
      "epoch 200, loss: 0.00032556717633269727\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "for epoch in range(300):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    padded_inputs = pad_sequences(inputs)\n",
    "    output = model(padded_inputs)\n",
    "    loss = loss_finction(output, targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch % 100) == 0:\n",
    "        print(f'epoch {epoch}, loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "617b0be9-e43c-437d-883e-c61b58ea121a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input: the animal didn't cross the street because it was too\n",
      "\n",
      "Target: tired\n",
      "\n",
      "Output: tired\n",
      "\n",
      "Input: the cat sat on the\n",
      "\n",
      "Target: mat\n",
      "\n",
      "Output: mat\n"
     ]
    }
   ],
   "source": [
    "# testing\n",
    "for seq, target in zip(input_data, target_data):\n",
    "    input_test = torch.tensor(seq, dtype=torch.long).unsqueeze(0)\n",
    "    model.eval()\n",
    "    output = model(input_test)\n",
    "    predictions = ix_to_word[torch.argmax(output).item()]\n",
    "    print(f'\\nInput: {\" \".join([ix_to_word[i] for i in seq])}')\n",
    "    print(f'\\nTarget: {ix_to_word[target]}')\n",
    "    print(f'\\nOutput: {predictions}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e60f5d-d663-43f9-8261-0dd70ede139f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
