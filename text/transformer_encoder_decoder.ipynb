{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c77da2cc-b05e-4cd5-97f8-dd47033c0b38",
   "metadata": {},
   "source": [
    "# Transformer Encoder-Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff6f454-e461-4bf5-ab9f-0b4340540e01",
   "metadata": {},
   "source": [
    "The encoder and decoder are connected via cross-attention layer.\n",
    "\n",
    "<img src=\"./img/encoder_decoder_cross_attn.png\" alt=\"encoder_decoder_cross_attn\" style=\"width: 400px;\"/>\n",
    "\n",
    "Cross-Attention layer is added at each decoder layer after the masked attention and takes a double input: the information processed through the decoder, and the final hidden states produced by the encoder, thereby linking the transformer's two main building blocks.\n",
    "\n",
    "This enables decoder to look-back at the input sequence and generate next in the target sequence.\n",
    "\n",
    "<img src=\"./img/translation_task.png\" alt=\"translation_task\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afd76b53-ccec-497c-b7c6-4e3b2e1e6484",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b7e59f64-5b00-4820-b18a-d0b7eb12596f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayerCrossAttn(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super().__init__()\n",
    "        # the causal (masked) self-attention and cross-attention\n",
    "        self.self_attention = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attention = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = FeedForwardTransformation(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, y, causal_mask, cross_mask):        \n",
    "        self_attn_output = self.self_attention(x,x,x,causal_mask)\n",
    "        x = self.norm1(x + self.dropout(self_attn_output))\n",
    "\n",
    "        # x - decoder information flow, becomes cross-attention query\n",
    "        # y - encoder output, becomes cross-attention key and values\n",
    "        cross_attn_output = self.cross_attention(x,y,y,cross_mask)\n",
    "        \n",
    "        x = self.norm2(x + self.dropout(cross_attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        \n",
    "        return x\n",
    "        \n",
    "\n",
    "class TransformerDecoderCrossAttn(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_seq_length):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder_embedding = torch.nn.Embedding(vocab_size, d_model)\n",
    "        self.decoder_embedding = torch.nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "        self.positional_encoding = PositionalEncoder(d_model, max_seq_length)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)]\n",
    "        )\n",
    "        # linear layer (head) for next-word prediction\n",
    "        self.fc = nn.Linear(d_model, vocab_size)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, y, causal_mask, cross_mask):\n",
    "        # Embeddings of input sequences\n",
    "        x = self.dropout(\n",
    "            self.positional_encoding(\n",
    "                self.encoder_embedding(x))\n",
    "        )\n",
    "        # Embeddings of output sequences\n",
    "        y = self.dropout(\n",
    "            self.positional_encoding(\n",
    "                self.decoder_embedding(y))\n",
    "        )\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            out = layer(x, y, causal_mask, cross_mask)\n",
    "            \n",
    "        # Apply the forward pass through the model head\n",
    "        out = self.fc(out)\n",
    "        return nn.functional.log_softmax(out, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23778626-ac31-4a16-a157-e17dfbc2fcba",
   "metadata": {},
   "source": [
    "The decoder only needs to take actual target sequences during training time. \n",
    "\n",
    "In translation, these would be examples of translations associated with the source-language sequences fed to the encoder. \n",
    "\n",
    "In text summarization, the output embeddings for the decoder are summarized versions of the input embeddings for the encoder, and so on.\n",
    "\n",
    "Words in the target sequence act as our training labels during the next-word generation process. At inference time, the decoder assumes the role of generating a target sequence, starting with an empty output embedding and gradually taking as its inputs the target words it is just generating on the fly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6576b0-e4e9-4625-9652-59f089f792ee",
   "metadata": {},
   "source": [
    "# Complete Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4e7990bb-fa6a-414a-ae66-a927c3b6e085",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trasformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_seq_length):\n",
    "        super().__init__()\n",
    "        self.encoder = TransformerEncoder(vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_seq_length)\n",
    "        self.decoder = TransformerDecoderCrossAttn(vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_seq_length)\n",
    "\n",
    "    def forward(self, src, src_mask, causal_mask):\n",
    "        encoder_out = self.encoder(src, src_mask)\n",
    "        decoder_out = self.decoder(src, encoder_out, causal_mask, mask)\n",
    "        return decoder_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1fe878a8-e43a-434e-9c86-893f0f35df12",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 3\n",
    "vocab_size = 10000\n",
    "batch_size = 8\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "d_ff = 2048\n",
    "sequence_length = 64\n",
    "dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eb1e1b7e-b002-41d1-9fbd-7a637590f2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input sequences\n",
    "input_sequence = torch.randint(0, vocab_size, (batch_size, sequence_length))\n",
    "padding_mask = torch.randint(0, 2, (sequence_length, sequence_length))\n",
    "causal_mask = torch.triu(torch.ones(sequence_length, sequence_length), diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7377fee4-50c7-4579-950f-d96b04301bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = TransformerEncoder(vocab_size, d_model, num_layers, num_heads, d_ff, dropout, sequence_length)\n",
    "decoder = TransformerDecoderCrossAttn(vocab_size, d_model, num_layers, num_heads, d_ff, dropout, sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cdcd37-348e-4268-a3bb-5b60a3314870",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_output = encoder(input_sequence, padding_mask)\n",
    "decoder_output = decoder(input_sequence, encoder_output, padding_mask, causal_mask)\n",
    "print(\"Batch's output shape: \", decoder_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3f03ab-ef79-4b9e-b9a0-33569ceb30c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
