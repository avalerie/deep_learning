{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c80a00b-6f02-4ef5-a0e1-cc70b644bbe4",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "## Text classification with CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fd4626-3445-4689-8403-5dc909c55ee8",
   "metadata": {},
   "source": [
    "Classification tasks can either be binary, multi-class, or multi-label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2d2dee-77cf-4176-8c96-6642551d0890",
   "metadata": {},
   "source": [
    "## Word embedding\n",
    "\n",
    "Representing words as numerical vectors that keeps semantic meaning and connections between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90320fb6-7759-42b3-aacb-aa957ecb2692",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "26858cca-e40e-44e8-8914-d6157ef9cc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence, label pairs\n",
    "text_samples = [\n",
    "    (\"I would recommend this book.\".split(),1),\n",
    "    (\"The story was interesting.\".split(),1),\n",
    "    (\"The plot is not written well\".split(),0),\n",
    "    (\"I like the characters\".split(),1),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f7ae8aa7-f4ae-43a1-b21e-a949ca42183e",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = {ii for i in text_samples for ii in i[0] }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a436c3fa-06e9-44e6-94b3-f493204c288a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'I',\n",
       " 'The',\n",
       " 'book.',\n",
       " 'characters',\n",
       " 'interesting.',\n",
       " 'is',\n",
       " 'like',\n",
       " 'not',\n",
       " 'plot',\n",
       " 'recommend',\n",
       " 'story',\n",
       " 'the',\n",
       " 'this',\n",
       " 'was',\n",
       " 'well',\n",
       " 'would',\n",
       " 'written'}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ef0469be-975a-43ac-b6eb-568774991361",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = {word: idx for idx, word in enumerate(words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "17b230fc-8359-451c-a580-d2e9bb1df600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'like': 0,\n",
       " 'The': 1,\n",
       " 'was': 2,\n",
       " 'interesting.': 3,\n",
       " 'written': 4,\n",
       " 'characters': 5,\n",
       " 'I': 6,\n",
       " 'not': 7,\n",
       " 'is': 8,\n",
       " 'would': 9,\n",
       " 'plot': 10,\n",
       " 'book.': 11,\n",
       " 'the': 12,\n",
       " 'story': 13,\n",
       " 'this': 14,\n",
       " 'well': 15,\n",
       " 'recommend': 16}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "adce17d8-6032-4dee-998f-2a85280c8190",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_words = [word_to_index[i] for i in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "14feeefb-6b5d-4add-a9f6-35491660734f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "85e701d6-45de-4e3a-ad3c-8a72acc255ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.LongTensor(preprocessed_words)\n",
    "embedding = torch.nn.Embedding(num_embeddings=len(words), embedding_dim=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "53592c48-c034-4272-9266-51baefc6b7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = embedding(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "21780a90-0a7d-4c45-bb8a-c45d80048034",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4200, -1.4579, -0.9933, -0.0159, -0.3277],\n",
       "        [-0.3111, -0.1781,  0.4281,  0.9825,  0.5980],\n",
       "        [ 0.0804, -0.4480, -0.9898,  0.4201, -1.2605],\n",
       "        [-1.2659, -0.6779,  0.5481,  0.7124, -2.3134],\n",
       "        [ 0.1346,  0.6053,  1.9627,  2.1202, -0.4237],\n",
       "        [-0.1819, -1.3125, -0.4762, -0.4572,  0.6339],\n",
       "        [-0.9884, -0.3030, -0.8270, -0.4089, -0.2556],\n",
       "        [ 0.5830, -0.1364,  0.3091, -1.2305,  0.1511],\n",
       "        [ 0.6196,  0.9154, -1.5264,  2.0294, -0.6247],\n",
       "        [ 0.7854, -2.4355,  0.4791,  0.3470,  0.6774],\n",
       "        [ 1.3339,  0.2000, -1.2083,  2.1884, -0.2339],\n",
       "        [-0.9382, -0.7047, -0.4650,  0.8876, -0.8559],\n",
       "        [ 0.9153, -0.3533,  0.1098,  1.6644, -0.2976],\n",
       "        [-0.5616, -1.2606, -0.3202,  0.5183,  1.0450],\n",
       "        [-1.1563, -1.1008, -0.4844, -0.8209,  0.1986],\n",
       "        [ 0.3050, -0.0158,  0.9197,  0.0267, -0.8344],\n",
       "        [ 1.0470, -0.9032,  0.6473,  1.0519, -0.7370]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9f2209-6463-41c9-904b-a3a219e09367",
   "metadata": {},
   "source": [
    "Output is an embedding vector for each input word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c34b226-1324-4a25-9124-b295fe837be0",
   "metadata": {},
   "source": [
    "## CNN - convolutional layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c176eb-7afd-494e-93a2-d20736af39ee",
   "metadata": {},
   "source": [
    "Convolutional layer detects patterns. \n",
    "\n",
    "The convolution operation is sliding the filter(kernel) over the input data and calculating element-wise matrix multiplication.\n",
    "\n",
    "The filter(kernel) is a small marix that slides over input.\n",
    "\n",
    "The stride is a number of positions the filter moves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e6fb3e-6142-4846-a587-82a9faf828f2",
   "metadata": {},
   "source": [
    "<img src=\"./img/cnn_conv.png\" alt=\"cnn_conv\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34707882-8d6a-4916-b347-fdb4865f6fcd",
   "metadata": {},
   "source": [
    "## CNN - pooling layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7a872b-0f0c-4c29-ad06-9f9898da80c9",
   "metadata": {},
   "source": [
    "Pooling layer reduces data size while preserving important information.\n",
    "\n",
    "<img src=\"./img/cnn_pooling.png\" alt=\"cnn_pooling.png\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecc12ea-f7b4-4770-9b24-424eb59608ea",
   "metadata": {},
   "source": [
    "## CNN - fully connected layer\n",
    "\n",
    "The last layer makes predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f80efc-e195-444b-aac8-2f8d2644e7eb",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dfb70f0f-7485-4f6b-8e48-4f2d43b385bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conv1d is prefered to Conv2d as text data is one dimentional\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embed_dim)\n",
    "        self.conv = torch.nn.Conv1d(embed_dim, embed_dim, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc = torch.nn.Linear(embed_dim, 2)\n",
    "\n",
    "    def forward(self, text):\n",
    "        # convert text to embedding\n",
    "        # reshape to cov layer shape (batch_size, embed_dim,sequence_len)\n",
    "        embed = self.embedding(text).permute(0, 2, 1)\n",
    "        # activation func - extracts features from embeddings\n",
    "        conved = torch.nn.functional.relu(self.conv(embed))\n",
    "        # calculate the average across the sequence length\n",
    "        conved = conved.mean(dim=2)\n",
    "        return self.fc(conved)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068bb406-379d-46c6-a2c5-f2e4eec49b09",
   "metadata": {},
   "source": [
    "By reducing the size of conv features dimension simplifies the information in each sentence to a single average value for easier analysis by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6a51d850-8a24-4c4e-a11d-bb7ddd747b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define embdedding shape\n",
    "vocab_size = len(word_to_index)\n",
    "embed_dim = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "794c9813-6169-4c05-adda-8962b881cb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(vocab_size, embed_dim)\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1e3dd606-65a7-4e51-938d-e1bc44765ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "for epoch in range(10):\n",
    "    for sentence, label in text_samples:\n",
    "        model.zero_grad()\n",
    "        sentence = torch.LongTensor([word_to_index[word] for word in sentence]).unsqueeze(0)\n",
    "        outputs = model(sentence)\n",
    "        label = torch.LongTensor([int(label)])\n",
    "        loss = loss_func(outputs, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48115e13-5f05-49e6-8e9f-7438f9d17190",
   "metadata": {},
   "source": [
    "We use unsqueeze zero to add an extra dimension to the start of the tensor, creating a batch containing a single sequence to fit the model's input expectations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "fa553d5a-34e5-4af6-b3dc-302c2702d14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples = [\n",
    "    (\"I like this story\".split()),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0d67c0fc-b942-4ea6-a6d5-2969836e5ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "for sentence in test_samples:\n",
    "    sentence = torch.LongTensor([word_to_index[word] for word in sentence]).unsqueeze(0)\n",
    "    outputs = model(sentence)\n",
    "    _, predicted_label = torch.max(outputs.data, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f263c41d-40d0-4001-99de-44ef0bf90a6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_label.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868f6ef3-b782-481d-b05a-2a51d54054c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
