{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3056e540-bce4-4c83-9b23-705aa76b184a",
   "metadata": {},
   "source": [
    "# Hugging Face Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd2637d-9b71-41c2-b607-1259be2af81e",
   "metadata": {},
   "source": [
    "## Text generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b04e84-4655-404b-9ae5-ffc9dce977a4",
   "metadata": {},
   "source": [
    "Hugging Face pretrained models:\n",
    "\n",
    "* GPT2LMHeadModel is used for text generation\n",
    "\n",
    "* GPT2Tokenizer\n",
    "  * converts text to tokens\n",
    "  * handles subword tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0d491e4-e7d4-426b-af68-d21354305ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de8373b2-8fc8-4241-87b6-0b0bba652517",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f89eabc-89be-48a9-af9c-0b3f31940840",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6500f420-965e-4da3-8b09-e7b9e1d8ef6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = 'The cat sat on the mat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52377b2a-5d66-46cf-a789-a2b3e152a1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flag return_tensors equals 'pt' specifies that we want these tensors in PyTorch format\n",
    "input_ids = tokenizer.encode(sample_text, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a74c45-3873-4bf0-9e35-0bc366ec56b4",
   "metadata": {},
   "source": [
    "Arguments:\n",
    "\n",
    "* temperature - controls the randomness of the output, with lower values reducing randomness\n",
    "* no_repeat_ngram_size parameter - prevents consecutive word repetition in the generated text\n",
    "* pad_token_id is set to the ID of the end-of-sentence (EOS) token, which means the model pads the output with this token if it's shorter than the maximum length of 40 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49c87093-32b0-4996-82c7-b678972c5133",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.generate(\n",
    "    input_ids, \n",
    "    max_length=40, \n",
    "    temperature=0.7, \n",
    "    no_repeat_ngram_size=2,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    do_sample=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42a028bc-d9fb-46c2-9230-5e6f5bb270a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The cat sat on the mat, its legs pulled under the covers and its paws folded back on its knees and held his paws. The cat's eyes went wide, the cat was still breathing heavily,\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f4be33-2ea9-40d2-948c-89e9057e9041",
   "metadata": {},
   "source": [
    "## Translation\n",
    "\n",
    "`t5-small` is Text-to-Text trasformer model. It supports English, French, Romanian, German."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3d941b5-053c-4291-ba25-213185321db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818d20cc-748e-41d2-b17a-db061cfcd669",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e639ffc0-2e84-467b-8dae-009918d06563",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = 'translate English to French: I love to read books'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61f3f1a0-26e3-4026-9591-78dc22ebe848",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer.encode(sample_text, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "622a0ce4-89f5-4e5d-b82a-a5347d59202b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.generate(input_ids, max_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea6eacdb-71db-4d86-b968-bc8336ffebc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Je lis des livres'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a88eb8d-9335-497b-aa6c-405ce80ed249",
   "metadata": {},
   "source": [
    "## Evaluating text generation with BLUE and ROUGE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28cb7d1-e28b-4880-96be-c0beed977839",
   "metadata": {},
   "source": [
    "BLEU and ROUGE compare generated text to reference texts and evaluate its quality more closely with how humans perceive language."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82028c11-4296-4aa3-adba-cc90bc5c23e5",
   "metadata": {},
   "source": [
    "**BLEU (Bilingual Evaluation Understudy)** compares the generated text with a reference text by examining the occurrence of n-grams. \n",
    "\n",
    "In a sentence like 'the cat is on the mat', the 1-grams or uni-grams are each individual word, the 2-grams or bi-grams are 'the cat', 'cat is', and so on. The more the generated n-grams match the reference n-grams, the higher the BLEU score. A perfect match results in a score of 1-point-0, while zero would mean no match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f6dc3930-af07-4ff7-9290-90ff0108fc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.text import BLEUScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bba770d3-b872-4d4b-bc98-03b3219e2283",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_text = ['the cat is on the mat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2d18139a-cee9-4535-9237-5a779dd09353",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_text = [['a cat is on the mat', 'there is a cat on mat']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aa9e8df0-ef55-4ab4-8a22-88910dee8495",
   "metadata": {},
   "outputs": [],
   "source": [
    "blue = BLEUScore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8b775fad-5605-4ab5-b2de-cf69b579eaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "blue_score = blue(generated_text, real_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "67727cf1-9237-45b1-b057-954049ad9aca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7598357200622559"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blue_score.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ff1f87-7830-47e7-8ff8-177315781f0b",
   "metadata": {},
   "source": [
    "**ROUGE (Recall-Oriented Understudy for Gisting Evaluation)** assesses generated text against reference text in two ways: \n",
    "\n",
    "* examines overlapping n-grams, with N representing the n-gram order\n",
    "* checks for the longest common subsequence (LCS), the longest shared word sequence between the generated and reference text\n",
    "\n",
    "ROUGE has three metrics:\n",
    "\n",
    "* F-measure is the harmonic mean of precision and recall. \n",
    "\n",
    "* Precision checks matches of n-grams in the generated text that are in the reference text (how many selected items are relevant). \n",
    "\n",
    "* Recall checks for matches of n-grams in the reference text that appear in the generated text (how many selected items are relevant). \n",
    "\n",
    "The prefixes 'rouge1', 'rouge2', and 'rougeL' specify the n-gram order or LCS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "54736462-1a00-4f98-ba5a-275c7804bd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.text import ROUGEScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b071f2f8-9278-40f0-851d-17fd527c0f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = ROUGEScore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "77f92206-a335-4a9f-9c87-441f3536bf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_score = rouge(generated_text, real_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8196c591-23ce-4d80-be35-386220aab7ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1_fmeasure': tensor(0.8333),\n",
       " 'rouge1_precision': tensor(0.8333),\n",
       " 'rouge1_recall': tensor(0.8333),\n",
       " 'rouge2_fmeasure': tensor(0.8000),\n",
       " 'rouge2_precision': tensor(0.8000),\n",
       " 'rouge2_recall': tensor(0.8000),\n",
       " 'rougeL_fmeasure': tensor(0.8333),\n",
       " 'rougeL_precision': tensor(0.8333),\n",
       " 'rougeL_recall': tensor(0.8333),\n",
       " 'rougeLsum_fmeasure': tensor(0.8333),\n",
       " 'rougeLsum_precision': tensor(0.8333),\n",
       " 'rougeLsum_recall': tensor(0.8333)}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0364d976-d6aa-4893-8c88-b725eeddb3d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
