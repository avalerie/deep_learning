{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b97d7a71-9b53-40e8-ab12-0e916f63fcb5",
   "metadata": {},
   "source": [
    "# Evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14290d55-9440-4d5d-a0d5-dfda2a124f7d",
   "metadata": {},
   "source": [
    "| Task | Accuracy | F1 | BLUE | Perplexity | ROUGE | EM | METEOR |\n",
    "| --- |:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n",
    "| Text classification |&check;|&check;| | | | | |\n",
    "| Text generation | | |&check;|&check;| | | |\n",
    "| Summarization | | |&check;| |&check;| | |\n",
    "| Translation | | |&check;| | | |&check;|\n",
    "| Extractive QA | |&check;| | | |&check;| |\n",
    "| Generative QA | |&check;| | |&check;| | |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70a1d943-5dbd-4c08-9c69-a33fdcf726df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "417a8886-6ac4-4288-b1b0-7988a9d85549",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4aca609-e305-4d95-abb0-ac3f25405344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The F1 score is the harmonic mean of the precision and recall. It can be computed with the equation:\n",
      "F1 = 2 * (precision * recall) / (precision + recall)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metric.description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2528ee7f-879e-4724-8b9c-a03288cde8f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predictions': Value(dtype='int32', id=None), 'references': Value(dtype='int32', id=None)}\n"
     ]
    }
   ],
   "source": [
    "# required inputs\n",
    "print(metric.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11fd63c1-df6a-4908-a75f-fdf2b6282241",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = [0,0,1]\n",
    "predicted_labels = [1,0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c9a6abb-cf9d-42f2-91d2-adc89a669529",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1': 0.6666666666666666}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric.compute(predictions=predicted_labels, references=true_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a88eb8d-9335-497b-aa6c-405ce80ed249",
   "metadata": {},
   "source": [
    "## BLUE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28cb7d1-e28b-4880-96be-c0beed977839",
   "metadata": {},
   "source": [
    "BLEU and ROUGE compare generated text to reference texts and evaluate its quality more closely with how humans perceive language."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82028c11-4296-4aa3-adba-cc90bc5c23e5",
   "metadata": {},
   "source": [
    "**BLEU (Bilingual Evaluation Understudy)** compares the generated text with a reference text by examining the occurrence of n-grams. \n",
    "\n",
    "In a sentence like 'the cat is on the mat', the 1-grams or uni-grams are each individual word, the 2-grams or bi-grams are 'the cat', 'cat is', and so on. The more the generated n-grams match the reference n-grams, the higher the BLEU score. A perfect match results in a score of 1-point-0, while zero would mean no match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f6dc3930-af07-4ff7-9290-90ff0108fc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.text import BLEUScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bba770d3-b872-4d4b-bc98-03b3219e2283",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_text = ['the cat is on the mat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2d18139a-cee9-4535-9237-5a779dd09353",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_text = [['a cat is on the mat', 'there is a cat on mat']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aa9e8df0-ef55-4ab4-8a22-88910dee8495",
   "metadata": {},
   "outputs": [],
   "source": [
    "blue = BLEUScore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8b775fad-5605-4ab5-b2de-cf69b579eaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "blue_score = blue(generated_text, real_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "67727cf1-9237-45b1-b057-954049ad9aca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7598357200622559"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blue_score.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973f26db-8a6b-4f6e-9bbd-bda528900a89",
   "metadata": {},
   "source": [
    "## ROUGE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ff1f87-7830-47e7-8ff8-177315781f0b",
   "metadata": {},
   "source": [
    "**ROUGE (Recall-Oriented Understudy for Gisting Evaluation)** assesses generated text against reference text in two ways: \n",
    "\n",
    "* examines overlapping n-grams, with N representing the n-gram order\n",
    "* checks for the longest common subsequence (LCS), the longest shared word sequence between the generated and reference text\n",
    "\n",
    "ROUGE has three metrics:\n",
    "\n",
    "* F-measure is the harmonic mean of precision and recall. \n",
    "\n",
    "* Precision checks matches of n-grams in the generated text that are in the reference text (how many selected items are relevant). \n",
    "\n",
    "* Recall checks for matches of n-grams in the reference text that appear in the generated text (how many selected items are relevant). \n",
    "\n",
    "The prefixes 'rouge1', 'rouge2', and 'rougeL' specify the n-gram order or LCS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "54736462-1a00-4f98-ba5a-275c7804bd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.text import ROUGEScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b071f2f8-9278-40f0-851d-17fd527c0f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = ROUGEScore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "77f92206-a335-4a9f-9c87-441f3536bf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_score = rouge(generated_text, real_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8196c591-23ce-4d80-be35-386220aab7ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1_fmeasure': tensor(0.8333),\n",
       " 'rouge1_precision': tensor(0.8333),\n",
       " 'rouge1_recall': tensor(0.8333),\n",
       " 'rouge2_fmeasure': tensor(0.8000),\n",
       " 'rouge2_precision': tensor(0.8000),\n",
       " 'rouge2_recall': tensor(0.8000),\n",
       " 'rougeL_fmeasure': tensor(0.8333),\n",
       " 'rougeL_precision': tensor(0.8333),\n",
       " 'rougeL_recall': tensor(0.8333),\n",
       " 'rougeLsum_fmeasure': tensor(0.8333),\n",
       " 'rougeLsum_precision': tensor(0.8333),\n",
       " 'rougeLsum_recall': tensor(0.8333)}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84389a8-51f1-4033-bdc8-f306477af29a",
   "metadata": {},
   "source": [
    "## Perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935020ba-aa1d-401c-9f2c-28228c88b429",
   "metadata": {},
   "source": [
    "It is defined as the exponentiated average negative log-likelihood of a sequence, calculated with exponent base `e`.\n",
    "\n",
    "This is a measurement of how well probability distribution or probability model predicts a sample. A lower perplexity score indicates better generalization performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e3843340-7336-471e-bb13-d764f754ad42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e016905b-15e5-450c-b266-3287da2ef197",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'gpt2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fadec724-3ee5-4280-b3dc-7546c8989925",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"\"\"\n",
    "Alice opened the door and found that it led into a small passage, not much larger than a rat-hole: \n",
    "she knelt down and looked along the passage into the loveliest garden you ever saw. \n",
    "How she longed to get out of that dark hall, and wander about among those beds of bright flowers and those cool fountains, \n",
    "but she could not even get her head though the doorway; `and even if my head would go through,' thought poor Alice,\n",
    "`it would be of very little use without my shoulders. Oh, how I wish I could shut up like a telescope! \n",
    "I think I could, if I only know how to begin.' For, you see, so many out-of-the-way things had happened lately, \n",
    "that Alice had begun to think that very few things indeed were really impossible.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "82181d94-b012-4af3-aa34-475fc36925e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Alice opened the door and found that it led to a small room with a large table. She sat down on it and\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer.encode(sample_text, return_tensors=\"pt\", max_length=10, truncation=True)\n",
    "outputs = model.generate(inputs, max_length=25)\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e7e601a3-16bd-4311-91c4-e6ee12e7e836",
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity = evaluate.load(\"perplexity\", module_type=\"metric\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "98603a09-e1cb-4bd3-9989-76da7c65d04a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'predictions': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0379e76b-7669-495b-af69-bb0fb567c0e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00fc5e421baa48658a32c3fe4d58f9a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = perplexity.compute(predictions=generated_text, model_id=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15af72b-11ae-43a6-86c2-1af758b92008",
   "metadata": {},
   "source": [
    "Perplexity is calculated by output logit distributions returned by the model to generate each next token.\n",
    "\n",
    "When multiple generated text predictions are passed, the average perplexity is beign used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "cf792179-c831-4948-949a-a9b87c0bf203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3786.449084772647"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['mean_perplexity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bbbee3-f302-4c09-a035-686d61236182",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
