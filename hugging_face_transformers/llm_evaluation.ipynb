{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b97d7a71-9b53-40e8-ab12-0e916f63fcb5",
   "metadata": {},
   "source": [
    "# Evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14290d55-9440-4d5d-a0d5-dfda2a124f7d",
   "metadata": {},
   "source": [
    "| Task | Accuracy | F1 | BLEU | Perplexity | ROUGE | EM | METEOR |\n",
    "| --- |:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n",
    "| Text classification |&check;|&check;| | | | | |\n",
    "| Text generation | | |&check;|&check;| | | |\n",
    "| Summarization | | |&check;| |&check;| | |\n",
    "| Translation | | |&check;| | | |&check;|\n",
    "| Extractive QA | |&check;| | | |&check;| |\n",
    "| Generative QA | |&check;| | |&check;| | |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a1d943-5dbd-4c08-9c69-a33fdcf726df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "417a8886-6ac4-4288-b1b0-7988a9d85549",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4aca609-e305-4d95-abb0-ac3f25405344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The F1 score is the harmonic mean of the precision and recall. It can be computed with the equation:\n",
      "F1 = 2 * (precision * recall) / (precision + recall)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metric.description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2528ee7f-879e-4724-8b9c-a03288cde8f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predictions': Value(dtype='int32', id=None), 'references': Value(dtype='int32', id=None)}\n"
     ]
    }
   ],
   "source": [
    "# required inputs\n",
    "print(metric.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11fd63c1-df6a-4908-a75f-fdf2b6282241",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = [0,0,1]\n",
    "predicted_labels = [1,0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c9a6abb-cf9d-42f2-91d2-adc89a669529",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1': 0.6666666666666666}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric.compute(predictions=predicted_labels, references=true_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a88eb8d-9335-497b-aa6c-405ce80ed249",
   "metadata": {},
   "source": [
    "## BLEU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28cb7d1-e28b-4880-96be-c0beed977839",
   "metadata": {},
   "source": [
    "BLEU and ROUGE compare generated text to reference texts and evaluate its quality more closely with how humans perceive language."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82028c11-4296-4aa3-adba-cc90bc5c23e5",
   "metadata": {},
   "source": [
    "**BLEU (Bilingual Evaluation Understudy)** compares the generated text with a reference text by examining the occurrence of n-grams. \n",
    "\n",
    "In a sentence like 'the cat is on the mat', the 1-grams or uni-grams are each individual word, the 2-grams or bi-grams are 'the cat', 'cat is', and so on. The more the generated n-grams match the reference n-grams, the higher the BLEU score. A perfect match results in a score of 1-point-0, while zero would mean no match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6dc3930-af07-4ff7-9290-90ff0108fc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.text import BLEUScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bba770d3-b872-4d4b-bc98-03b3219e2283",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_text = ['the cat is on the mat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d18139a-cee9-4535-9237-5a779dd09353",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_text = [['a cat is on the mat', 'there is a cat on mat']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa9e8df0-ef55-4ab4-8a22-88910dee8495",
   "metadata": {},
   "outputs": [],
   "source": [
    "blue = BLEUScore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b775fad-5605-4ab5-b2de-cf69b579eaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "blue_score = blue(generated_text, real_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67727cf1-9237-45b1-b057-954049ad9aca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7598357200622559"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blue_score.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6012a2ef-f15a-4230-a1f2-a9dbf4947781",
   "metadata": {},
   "source": [
    "## BLEU translation task\n",
    "\n",
    "In translation tasks, BLEU measures translation quality of LLM outputs against provided references.\n",
    "\n",
    "The example below loads the BLEU score and a Spanish to English translation pipeline to evaluate the generated output against two references. Notice how the single translated text is encapsulated as a list before being passed as the predictions argument of the metric's compute method. \n",
    "\n",
    "BLEU reports an overall similarity score as well as several domain-specific measurements, such as precisions for n-grams of different length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dffe94-1ad3-469a-9ffb-9969b1f8b0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "blue = evaluate.load(\"bleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a626316c-d6cf-4792-9077-61b0b0cc5a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model_name = \"Helsinki-NLP/opus-mt-es-en\"\n",
    "translator = pipeline(\"translation\", model=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3eea3425-a107-41f6-a362-a3532d762898",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"Ha estado lloviendo todo el dia\"\n",
    "references = [[\"It's raining all day\", \"It's been raining all day\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "14633d18-47eb-4728-a7ec-3234ebca0e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's been raining all day.\n"
     ]
    }
   ],
   "source": [
    "translated_output = translator(sample_text)\n",
    "sentence = translated_output[0][\"translation_text\"]\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "93707beb-bbcd-4a37-bb56-f9b22f94b187",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bleu': 0.7598356856515925,\n",
       " 'precisions': [0.8333333333333334, 0.8, 0.75, 0.6666666666666666],\n",
       " 'brevity_penalty': 1.0,\n",
       " 'length_ratio': 1.5,\n",
       " 'translation_length': 6,\n",
       " 'reference_length': 4}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = blue.compute(predictions=[sentence], references=references)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84389a8-51f1-4033-bdc8-f306477af29a",
   "metadata": {},
   "source": [
    "## Perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935020ba-aa1d-401c-9f2c-28228c88b429",
   "metadata": {},
   "source": [
    "It is defined as the exponentiated average negative log-likelihood of a sequence, calculated with exponent base `e`.\n",
    "\n",
    "This is a measurement of how well probability distribution or probability model predicts a sample. A lower perplexity score indicates better generalization performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3843340-7336-471e-bb13-d764f754ad42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e016905b-15e5-450c-b266-3287da2ef197",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'gpt2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fadec724-3ee5-4280-b3dc-7546c8989925",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"\"\"\n",
    "Alice opened the door and found that it led into a small passage, not much larger than a rat-hole: \n",
    "she knelt down and looked along the passage into the loveliest garden you ever saw. \n",
    "How she longed to get out of that dark hall, and wander about among those beds of bright flowers and those cool fountains, \n",
    "but she could not even get her head though the doorway; `and even if my head would go through,' thought poor Alice,\n",
    "`it would be of very little use without my shoulders. Oh, how I wish I could shut up like a telescope! \n",
    "I think I could, if I only know how to begin.' For, you see, so many out-of-the-way things had happened lately, \n",
    "that Alice had begun to think that very few things indeed were really impossible.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82181d94-b012-4af3-aa34-475fc36925e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Alice opened the door and found that it led to a small room with a large table. She sat down on it and\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer.encode(sample_text, return_tensors=\"pt\", max_length=10, truncation=True)\n",
    "outputs = model.generate(inputs, max_length=25)\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7e601a3-16bd-4311-91c4-e6ee12e7e836",
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity = evaluate.load(\"perplexity\", module_type=\"metric\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "98603a09-e1cb-4bd3-9989-76da7c65d04a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'predictions': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0379e76b-7669-495b-af69-bb0fb567c0e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ed1d84dc96e4bd29ce73d49c9a6a3b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = perplexity.compute(predictions=generated_text, model_id=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15af72b-11ae-43a6-86c2-1af758b92008",
   "metadata": {},
   "source": [
    "Perplexity is calculated by output logit distributions returned by the model to generate each next token.\n",
    "\n",
    "When multiple generated text predictions are passed, the average perplexity is beign used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cf792179-c831-4948-949a-a9b87c0bf203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3786.449084772647"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['mean_perplexity']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973f26db-8a6b-4f6e-9bbd-bda528900a89",
   "metadata": {},
   "source": [
    "## ROUGE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ff1f87-7830-47e7-8ff8-177315781f0b",
   "metadata": {},
   "source": [
    "**ROUGE (Recall-Oriented Understudy for Gisting Evaluation)** assesses generated text against reference text in two ways: \n",
    "\n",
    "* examines overlapping n-grams, with N representing the n-gram order\n",
    "* checks for the longest common subsequence (LCS), the longest shared word sequence between the generated and reference text (overlapping)\n",
    "\n",
    "ROUGE has three metrics:\n",
    "\n",
    "* F-measure is the harmonic mean of precision and recall. \n",
    "\n",
    "* Precision checks matches of n-grams in the generated text that are in the reference text (how many selected items are relevant). \n",
    "\n",
    "* Recall checks for matches of n-grams in the reference text that appear in the generated text (how many selected items are relevant). \n",
    "\n",
    "The prefixes 'rouge1', 'rouge2', and 'rougeL' specify the n-gram order or LCS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "54736462-1a00-4f98-ba5a-275c7804bd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.text import ROUGEScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b071f2f8-9278-40f0-851d-17fd527c0f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = ROUGEScore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "77f92206-a335-4a9f-9c87-441f3536bf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_score = rouge(generated_text, real_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8196c591-23ce-4d80-be35-386220aab7ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1_fmeasure': tensor(0.2069),\n",
       " 'rouge1_precision': tensor(0.1304),\n",
       " 'rouge1_recall': tensor(0.5000),\n",
       " 'rouge2_fmeasure': tensor(0.),\n",
       " 'rouge2_precision': tensor(0.),\n",
       " 'rouge2_recall': tensor(0.),\n",
       " 'rougeL_fmeasure': tensor(0.1379),\n",
       " 'rougeL_precision': tensor(0.0870),\n",
       " 'rougeL_recall': tensor(0.3333),\n",
       " 'rougeLsum_fmeasure': tensor(0.1379),\n",
       " 'rougeLsum_precision': tensor(0.0870),\n",
       " 'rougeLsum_recall': tensor(0.3333)}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ed4cd9-24af-43fa-9f44-c2d7a3131574",
   "metadata": {},
   "source": [
    "## METEOR translation task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f14d493-3098-4bfc-9e50-73467c619a7a",
   "metadata": {},
   "source": [
    "Metric for Evaluation of Translation with Explicit Ordering (METEOR) score is a metric that measures the quality of generated text based on the alignment between the generated text and the reference text.\n",
    "\n",
    "METEOR was proposed to overcome some limitations in ROUGE and BLEU by incorporating more linguistic aspects in the evaluation, such as stemming to deal with morphological variations, capturing words with similar meanings, and penalizing errors in word order. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91c8d3e-2241-4df8-9dc2-e27ea6553238",
   "metadata": {},
   "source": [
    "## EM (exact match) question answering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e49193-fe49-4c8c-bde1-ba93f7b88552",
   "metadata": {},
   "source": [
    "EM returns 1 when the model output exactly matches its associated reference answer, and 0 otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea515bb-6fe0-46f2-aaa6-342c10129716",
   "metadata": {},
   "source": [
    "# Metrics for analyzing bias\n",
    "\n",
    "**TOXICITY** is a metric to quantify language toxicity by using a pre-trained classification LLM for detecting hate speech. \n",
    "\n",
    "It takes a list of one or more texts as input, and calculates a toxicity score between 0 and 1 per input, or returns the maximum of the inputs' toxicity scores if the argument 'aggregation=\"maximum\"' is specified, as shown in this example. Alternatively, it can also return the percentage of input predictions with a toxicity score above 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc2b72b-fae3-41bc-bf20-8407c87251bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity_metric = evaluate.load('toxicity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1dfe881f-102e-4c0c-bf65-ad17e1cc9eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_1 = [\"Everyone likes sunny days\", \"Everyone would relate to this\"]\n",
    "text_2 = [\"The random person\", \"This person is opionated\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d1ac063a-5ce0-42de-8bc1-f9b573df4919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_toxicity': 0.00017157204274553806}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = toxicity_metric.compute(predictions=text_1, aggregation=\"maximum\")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c9264183-dd12-4d4b-877c-71f96596a376",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_toxicity': 0.06041119620203972}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = toxicity_metric.compute(predictions=text_2, aggregation=\"maximum\")\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11bf707-3371-4b7c-b211-c724db02b497",
   "metadata": {},
   "source": [
    "**REGARD** quantifies language polarity and biased perception towards certain demographics or groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7b2a9a-b76a-418e-b38a-a6b926d609cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "regard_metric = evaluate.load('regard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "58b6c4d9-06a8-4cc0-bfe7-5d5632e85aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = regard_metric.compute(data=text_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5f3846c0-5fc1-4868-8aa5-0332be10523c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = regard_metric.compute(data=text_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c6c8a1cc-5731-4a59-aa84-8e4bff56d2b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'regard': [[{'label': 'neutral', 'score': 0.9326518177986145},\n",
       "   {'label': 'negative', 'score': 0.03637849912047386},\n",
       "   {'label': 'positive', 'score': 0.017817307263612747},\n",
       "   {'label': 'other', 'score': 0.01315248478204012}],\n",
       "  [{'label': 'neutral', 'score': 0.9218319654464722},\n",
       "   {'label': 'negative', 'score': 0.04694298654794693},\n",
       "   {'label': 'other', 'score': 0.01583946868777275},\n",
       "   {'label': 'positive', 'score': 0.015385557897388935}]]}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa01afa-066f-4ddc-9861-ce6fa1d67488",
   "metadata": {},
   "source": [
    "The second sentence in the last sample has higher negativity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c428899-e143-4ab7-a7c5-26202bc402db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
