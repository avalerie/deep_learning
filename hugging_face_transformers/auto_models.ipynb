{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40d04f2e-c5c8-41ef-8365-66622ec27864",
   "metadata": {},
   "source": [
    "# Auto class for text summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d401bb9-7055-4de4-94bd-12b55fcd0927",
   "metadata": {},
   "source": [
    "Training a model for this task requires input-target sequence pairs.\n",
    "\n",
    "* Input: original text\n",
    "* Target: summarized text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf05377-ac7a-4c87-bae2-96d1e25e609b",
   "metadata": {},
   "source": [
    "Extractive summarization, extracts and combines parts of the original text to create a summary, by using encoder models like BERT or sometimes encoder-decoder models like T5.\n",
    "\n",
    "Abstractive or Generative summarization relies on sequence-to-sequence LLMs to generate (word by word) a summary that may use different words and sentence structures than those in the original text.\n",
    "\n",
    "\n",
    "<p float=\"left\">\n",
    "  <img align=\"left\" src=\"./img/extractive_sum.png\" alt=\"extractive_sum\" style=\"width: 400px;\"/>\n",
    "  <img align=\"left\" hspace=\"100\" src=\"./img/abstractive_sum.png\" alt=\"abstractive_sum\" style=\"width: 400px;\"/>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d6a7cfe-0580-414b-b5c3-5ab1cb056cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18ae79ca-f351-40f4-be22-c11fe567248c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 't5-small'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb8d5ef3-e424-4018-8805-03ea4406ff1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"\"\"\n",
    "Alice opened the door and found that it led into a small passage, not much larger than a rat-hole: \n",
    "she knelt down and looked along the passage into the loveliest garden you ever saw. \n",
    "How she longed to get out of that dark hall, and wander about among those beds of bright flowers and those cool fountains, \n",
    "but she could not even get her head though the doorway; `and even if my head would go through,' thought poor Alice,\n",
    "`it would be of very little use without my shoulders. Oh, how I wish I could shut up like a telescope! \n",
    "I think I could, if I only know how to begin.' For, you see, so many out-of-the-way things had happened lately, \n",
    "that Alice had begun to think that very few things indeed were really impossible.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ef316f5-09fa-41b6-9fc0-d99480ef4b1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"the doorway, she knelt down and found that it led into a small passage, not much larger than a rat-hole. and even if my head would go through,'\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer.encode(sample_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "outputs = model.generate(inputs, max_length=50)\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27dbc9bd-98e9-4ac0-a9e7-29a32f858f48",
   "metadata": {},
   "source": [
    "# Auto class for text generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bc38c0-fbb4-498e-a796-931e78c1746c",
   "metadata": {},
   "source": [
    "Next-word prediction is a form of self-supervised task requiring training examples consisting of input-target sequence pairs. \n",
    "\n",
    "* Input sequence is a segment of a text. For example, \"the cat is\", from \"the cat is sleeping on the mat\".\n",
    "* Target sequence are the tokens shifted one position to the left, e.g. \"cat is sleeping on the mat\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "234a3876-5344-4310-90f4-cd86816026f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9ecf991-8a0e-4f59-aea0-163aa7e213fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'gpt2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78d746c0-52b0-4496-9981-5d9f87f4111a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"The grey cat sat on the orange mat and was looking very sad. It was dreaming about \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d75fe87-f58c-441e-80bf-6de69fc82df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.encode(sample_text, return_tensors=\"pt\")\n",
    "outputs = model.generate(inputs, max_length=50)\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0a6db73-2ee7-4659-9a4d-587fc899baf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The grey cat sat on the orange mat and was looking very sad. It was dreaming about  the day he would be born.\n",
      "\"I'm going to be a cat,\" he said.\n",
      "\"I'm going to be a cat,\"\n"
     ]
    }
   ],
   "source": [
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b279de-827c-49d0-b3c4-2906a58d2ec3",
   "metadata": {},
   "source": [
    "# Auto class for text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523a56af-a74a-4247-b9f8-9165e7502c03",
   "metadata": {},
   "source": [
    "AutoModelForSequenceClassification\n",
    "\n",
    "BERT-based model for sentiment classification in a 5-star rating scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d42e546-a885-4734-8c0e-ad60e323ee92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13beedcb-43e6-4985-b807-ffd5953c0e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'nlptown/bert-base-multilingual-uncased-sentiment'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa5e178f-42de-4e7a-b0ab-049e19cc6db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"I like the colour of this product\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39d8858f-247c-4876-a09a-f2381adc1e90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(sample_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=64)\n",
    "outputs = model(**inputs)\n",
    "predicted_class = torch.argmax(outputs.logits, dim=1).item()\n",
    "predicted_class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4008baf2-dc40-4d8b-b521-058cf49cb3fb",
   "metadata": {},
   "source": [
    "# Auto class for QA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a0d550-3ddb-4f1b-a77e-a09355ec98b0",
   "metadata": {},
   "source": [
    "Qustion-Answering could have varios architectures:\n",
    "\n",
    "* Encoder-only for extractive QA.\n",
    "* Encoder-Decoder for abstractive QA;\n",
    "* Decoder-only for closed QA when LLM generates the answer with no context provided.\n",
    "\n",
    "The typical dataset features are: 'context', 'question', 'answers'.\n",
    "\n",
    "Extractive QA is formulated as supervised classification problem.\n",
    "\n",
    "The pre-processed question and context are jointly passed as input to the LLM which returns some raw outputs or logits. \n",
    "\n",
    "There are two output logits generated for each input token in the input sequence, indicating the likelihood that the token constitutes the start or end position of the answer span. \n",
    "\n",
    "Raw logits are post-processed to obtain the actual prediction or answer span: a portion of the input sequence defined by start and end token positions which are most likely containing the answer. \n",
    "\n",
    "This answer span is obtained as the positions of the start and end logits with the highest combined likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60a1577a-0ca0-46fc-8ac8-d25bdd7b5bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"\n",
    "In the ten years up  to the start of the financial crisis, house prices tripled. \n",
    "Many people think this is because there were not enough houses around, but that is only part of the picture.\n",
    "House prices rise much faster than wages, which means that houses become less and less affordable. \n",
    "Anyone who didn’t already own a house before the bubble started growing ends up giving up more and more of their \n",
    "salary simply to pay for a place to live. \n",
    "And it’s not just house buyers who are affected: pretty soon rents go up too, including in social housing.\n",
    "This increase in prices led to a massive increase in the amount of money that first time buyers spent on mortgage repayments.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40af3697-77ae-45e9-a065-6ecf1962ee58",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'Why are the house prices so high?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea0efcb0-5b88-4bac-9e35-cd14da8f0ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForQuestionAnswering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f68e811-a8e0-44ad-8295-8bfcda72dd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'deepset/minilm-uncased-squad2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "69657b30-0e9a-454a-ac4c-8bd624f897dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(question, context, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9fc96104-e79e-47f8-8255-7eb1e96701a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ef8650-4d3a-4617-8349-d9e678987ccd",
   "metadata": {},
   "source": [
    "Inputs contain the input_ids and attention_mask tensors and token_type_ids tensor whose values are 0 for tokens belonging to the question, and 1 for tokens from the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c3cd9e18-1f5e-4d28-b634-235be95fdcd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'there were not enough houses around'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# positions of input delimiting asnwer span\n",
    "start_idx = torch.argmax(outputs.start_logits)\n",
    "end_idx = torch.argmax(outputs.end_logits) + 1\n",
    "\n",
    "answer = tokenizer.decode(\n",
    "    inputs[\"input_ids\"][0][start_idx:end_idx]\n",
    ")\n",
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6d9eb8-d8ec-494d-a984-7339ab649ec3",
   "metadata": {},
   "source": [
    "# Auto class for translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16be9cff-fdd4-4014-add9-f80a572cb144",
   "metadata": {},
   "source": [
    "Training a model for this task requires input-target sequence pairs.\n",
    "\n",
    "* Input: text in source language\n",
    "* Target: translated text\n",
    "\n",
    "Translation is normally possible thanks to encoder-decoder models, such as the original transformer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107a58a3-89dc-448f-8065-ccb30ff75c15",
   "metadata": {},
   "source": [
    "# AutoModel general class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3dd952de-3706-457f-b6e4-9021e217380a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa138b2d-3481-4d07-bbe5-84e25b8351d2",
   "metadata": {},
   "source": [
    "AutoModel is a generic class that, when being passed some inputs for inference, returns the hidden states produced by the model body, but it lacks a task-specific head. \n",
    "\n",
    "It should be included at the end as the classification head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "815c4172-d904-418f-a739-745f5043ff66",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fb88e0ef-b50a-4127-9e68-2a065002d8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"I like summer long days\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "616ee0cb-bc86-451d-9217-bdf44776363f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassification(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(input_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "279cb9c1-2e84-4344-9698-f9fcd024cad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize inputs\n",
    "inputs = tokenizer(\n",
    "    sample_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eb899fd0-b3f8-4c7a-b1b6-6c2a3afabdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b657c8-344e-4d84-91f9-a05a48204672",
   "metadata": {},
   "source": [
    "Model hidden states:\n",
    "\n",
    "* `pooler_output` - high-level aggregated representation of the sequence\n",
    "* `last_hidden_state` - raw unaggregated hidden states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "58a65b11-d8c8-4cd0-a1ec-de48909a6a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pooled output size:  torch.Size([1, 768])\n",
      "Last hidden state size:  torch.Size([1, 7, 768])\n"
     ]
    }
   ],
   "source": [
    "# hidden states\n",
    "pooled_output = outputs.pooler_output\n",
    "print('Pooled output size: ', pooled_output.shape)\n",
    "print('Last hidden state size: ', outputs.last_hidden_state.shape)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0ca0a5a5-1003-4b30-ae29-2d422971e2e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6490, 0.3510]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_head = TextClassification(pooled_output.size(-1), 2)\n",
    "logits = classifier_head(pooled_output)\n",
    "# class probabilities\n",
    "proba = torch.softmax(logits, dim=1)\n",
    "proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a7e0ec-5787-4592-b3c7-31b762906f9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
