{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf4ca644-6660-4b38-8266-f5d4d710da78",
   "metadata": {},
   "source": [
    "# RLHF\n",
    "\n",
    "Reinforcement Learning from Human Feedback\n",
    "\n",
    "* Initial LLM\n",
    "* Train a Reward Model(RM)\n",
    "* Optimize(fine-tune)LLM using RL(e.g. PPO) based on trained RM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef5a595-bd48-4067-a99a-eba9bbf52900",
   "metadata": {},
   "source": [
    "## Reward Model\n",
    "\n",
    "This is a separate model which is calibrated with human preferences. The model predicts rewards for LLM input-outputs.\n",
    "\n",
    "The model is used by RL to fine-tune LLM.\n",
    "\n",
    "* Collect samples(inputs and outputs) from pre-trained LLM\n",
    "* Human annotatatates samples that used in scoring and ranking\n",
    "* Build training dataset as sample-reward pair\n",
    "* Train Reward Model that learns from human preferences on LLM outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2519c517-53b7-4394-8e00-f779c43eaa44",
   "metadata": {},
   "source": [
    "## TRL \n",
    "\n",
    "Transformer Reinforcement Learning library accommodates several RL approaches to fine-tune transformer-based LLMs.\n",
    "\n",
    "Proximal Policy Optimization (PPO)  optimizes LLM using <prompt, reponse, reward > triplets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686d57cb-45a4-4f8a-85ca-bbd5bf673d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import PPOTrainer, PPOConfig, create_reference_model, AutoModelForCausalLMWithValueHead\n",
    "from trl.core import respons_to_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cdf6c1-1db0-42ab-b2eb-a0d941bcf17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLMWithValueHead.from_pretrained(\"gpt2\")\n",
    "\n",
    "# refrence of loaded pre-trained model before optimization\n",
    "model_ref = create_reference_model(model)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac435cf-c838-4040-8875-18ddbdab6f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'My plan for today was '\n",
    "sample_text = tokenizer.encode(query_txt, return_tensor='pt')\n",
    "# respons_to_batch is same as model.generate() in RL library\n",
    "response = respons_to_batch(model, sample_text)\n",
    "\n",
    "ppo_config = PPOConfig(batcch_size=1)\n",
    "ppo_trainer = PPOTrainer(ppo_config, model, model_ref, tokenizer)\n",
    "reward = [torch.tensor(1.0)]\n",
    "train_stats = ppo_trainer.step([sample_text[0]], [response[0]], reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89360057-81e8-43ea-8237-d85fd2d57315",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
