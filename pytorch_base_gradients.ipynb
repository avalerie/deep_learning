{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMa/JBmkZhbHyrT/CavceE5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/whitestones011/deep_learning/blob/master/pytorch_base_gradients.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vanishing & exploding gradients"
      ],
      "metadata": {
        "id": "vhnYpiym1539"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Vanishing* -> gradients become infinitely smaller during backpropagation, so the earlier layers receive small update and model doesn't learn.\n",
        "\n",
        "*Exploding* -> gradients are getting larger resulting in large parameter update, so training diverges."
      ],
      "metadata": {
        "id": "fAoSR35b19y1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Solutions for unstable gradients:\n",
        "\n",
        "* Weight initialization;\n",
        "\n",
        "* Activation function;\n",
        "\n",
        "* Batch normalization."
      ],
      "metadata": {
        "id": "ARg_AccL3OkG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Weight initialization\n",
        "\n",
        "Solution for good weight initialization ensures that:\n",
        "\n",
        "* Comparable variance of input and output layers;\n",
        "\n",
        "* Varience of gradients the same before and after a layer.\n",
        "\n",
        "\n",
        "The choice of initialization method depends on activation function.\n",
        "\n",
        "For example: ReLU -> He/Kaiming initilization."
      ],
      "metadata": {
        "id": "YtU0qKTm3pXa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn"
      ],
      "metadata": {
        "id": "9MGoGPpL15VF"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "# default\n",
        "linear = nn.Linear(2, 4)\n",
        "linear.weight"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvWYK1bA5zEj",
        "outputId": "ffa286aa-3b05-48f0-a5d9-b42e85246134"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[-0.0053,  0.3793],\n",
              "        [-0.5820, -0.5204],\n",
              "        [-0.2723,  0.1896],\n",
              "        [-0.0140,  0.5607]], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# kaiming\n",
        "torch.manual_seed(0)\n",
        "nn.init.kaiming_uniform_(linear.weight)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7aj3SR_5pam",
        "outputId": "5501d961-e7ba-4f8f-e6cf-2e240d6a0db4"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[-0.0130,  0.9291],\n",
              "        [-1.4256, -1.2747],\n",
              "        [-0.6671,  0.4645],\n",
              "        [-0.0343,  1.3733]], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(9, 16)\n",
        "        self.fc2 = nn.Linear(16, 8)\n",
        "        self.fc3 = nn.Linear(8, 1)\n",
        "\n",
        "        # Apply He initialization\n",
        "        nn.init.kaiming_uniform_(self.fc1.weight)\n",
        "        nn.init.kaiming_uniform_(self.fc2.weight)\n",
        "        nn.init.kaiming_uniform_(self.fc3.weight, nonlinearity=\"sigmoid\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Update ReLU activation to ELU\n",
        "        x = nn.functional.elu(self.fc1(x))\n",
        "        x = nn.functional.elu(self.fc2(x))\n",
        "        x = nn.functional.sigmoid(self.fc3(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "jEDDZyN-_DA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Batch normalization\n",
        "Batch normalization tends to accelerate training convergence and protects the model from vanishing and exploding gradients issues.\n",
        "\n",
        "Normalise layer output:\n",
        "\n",
        "* Substract the mean;\n",
        "* Divide by standard deviation;\n",
        "* Scale and shift the inputs using learned parameters.\n",
        "\n"
      ],
      "metadata": {
        "id": "yfe_xbFK-ClT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(9, 16)\n",
        "        self.fc2 = nn.Linear(16, 8)\n",
        "        self.fc3 = nn.Linear(8, 1)\n",
        "        # Add two batch normalization layers\n",
        "        self.bn1 = nn.BatchNorm1d(16)\n",
        "        self.bn2 = nn.BatchNorm1d(8)\n",
        "\n",
        "        nn.init.kaiming_uniform_(self.fc1.weight)\n",
        "        nn.init.kaiming_uniform_(self.fc2.weight)\n",
        "        nn.init.kaiming_uniform_(self.fc3.weight, nonlinearity=\"sigmoid\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = nn.functional.elu(x)\n",
        "\n",
        "        # Pass x through the second set of layers\n",
        "        x = self.fc2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = nn.functional.elu(x)\n",
        "\n",
        "        x = nn.functional.sigmoid(self.fc3(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "Gp89sQts8Wxc"
      },
      "execution_count": 54,
      "outputs": []
    }
  ]
}