{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-output model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/multi_task_vs_multi_label.png\" alt=\"multi_task_vs_multi_label\" style=\"width: 600px;\"/>\n",
    "\n",
    "Multi-task model vs Multi-label classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Omniglot dataset - collection of images of 964 hand-written characters from 30 alphabets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets import Omniglot\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def find_classes(root_dir):\n",
    "    retour = []\n",
    "    for (root, dirs, files) in os.walk(root_dir):\n",
    "        for f in files:\n",
    "            if (f.endswith(\"png\")):\n",
    "                r = root.split('/')\n",
    "                lr = len(r)\n",
    "                retour.append((r[lr - 2], r[lr - 2] + \"/\" + r[lr - 1], root + \"/\" + f))\n",
    "    print(\"== Found %d items \" % len(retour))\n",
    "    return retour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Found 19280 items \n"
     ]
    }
   ],
   "source": [
    "img_samples = find_classes('./data/omniglot-py/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Japanese_(hiragana)',\n",
       " 'Japanese_(hiragana)/character05',\n",
       " './data/omniglot-py/images_background/Japanese_(hiragana)/character05/0492_07.png')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_samples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet_codes, alphabet = pd.Series([i[0] for i in img_samples]).factorize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Japanese_(hiragana)', 'Inuktitut_(Canadian_Aboriginal_Syllabics)',\n",
       "       'Malay_(Jawi_-_Arabic)', 'Ojibwe_(Canadian_Aboriginal_Syllabics)',\n",
       "       'N_Ko', 'Korean', 'Futurama', 'Arcadian', 'Sanskrit', 'Grantha',\n",
       "       'Burmese_(Myanmar)', 'Early_Aramaic', 'Greek', 'Cyrillic', 'Tifinagh',\n",
       "       'Latin', 'Bengali', 'Balinese', 'Braille', 'Tagalog', 'Gujarati',\n",
       "       'Japanese_(katakana)', 'Anglo-Saxon_Futhorc', 'Asomtavruli_(Georgian)',\n",
       "       'Mkhedruli_(Georgian)', 'Hebrew', 'Alphabet_of_the_Magi',\n",
       "       'Blackfoot_(Canadian_Aboriginal_Syllabics)', 'Armenian',\n",
       "       'Syriac_(Estrangelo)'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphabet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_codes, labels = pd.Series([i[1] for i in img_samples]).factorize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(964, 30)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels), len(alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [[img_path[2], alphabet, label] for img_path, alphabet, label in zip(img_samples, alphabet_codes, labels_codes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./data/omniglot-py/images_background/Japanese_(hiragana)/character05/0492_07.png',\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom Dataset class\n",
    "class OmniglotDataset(Dataset):\n",
    "    def __init__(self, transform, samples):\n",
    "        self.transform = transform\n",
    "        self.samples = samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label_alph, label_char = self.samples[idx]\n",
    "        img = Image.open(img_path).convert('L')\n",
    "        img = self.transform(img)\n",
    "        return img, label_alph, label_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = OmniglotDataset(\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((64, 64)),\n",
    "    ]),\n",
    "    samples=samples,\n",
    ")\n",
    "\n",
    "dataloader_train = DataLoader(\n",
    "    dataset_train, shuffle=True, batch_size=32,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building two-output model to predict character and alphabet\n",
    "\n",
    "<img src=\"./img/multi_output.png\" alt=\"multi_output\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()        \n",
    "        self.image_layer = nn.Sequential(            \n",
    "            nn.Conv2d(1, 16, kernel_size=3, padding=1),            \n",
    "            nn.MaxPool2d(kernel_size=2),            \n",
    "            nn.ELU(),            \n",
    "            nn.Flatten(),            \n",
    "            nn.Linear(16*32*32, 128)        \n",
    "        )              \n",
    "        self.class_alphabet = nn.Linear(128, 30)\n",
    "        self.class_character = nn.Linear(128, 964)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.image_layer(x)\n",
    "        out_alphabet = self.class_alphabet(x)\n",
    "        out_character = self.class_character(x)\n",
    "        return out_alphabet,out_character\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(net.parameters(), lr=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for img, lables_alph, labels_char in dataloader_train:\n",
    "        optimizer.zero_grad()\n",
    "        out_alph, out_char = net(img)\n",
    "        loss_alph = loss_function(out_alph, lables_alph)\n",
    "        loss_char = loss_function(out_char, labels_char)\n",
    "        total_loss = 0.3 * loss_alph + 0.7 * loss_char # add weight to loss - optimization \n",
    "        total_loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss funtions ideally should be on the same scale.\n",
    "\n",
    "If scale is different, there will be penalization of smaller scale loss.\n",
    "\n",
    "The solution: normalize losses before weighting and adding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_price = loss_price / torch.max(loss_price) # MSE oss\n",
    "loss_quality = loss_quality / torch.max(loss_quality) # CrossEntropy loss\n",
    "total = 0.3 * loss_price + 0.7 * loss_quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set metric for each output\n",
    "acc_alph = Accuracy(\n",
    "    task='multiclass', num_classes=30\n",
    ")\n",
    "acc_char = Accuracy(\n",
    "    task='multiclass', num_classes=964\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation loop\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    for img, lables_alph, labels_char in dataloader_test:\n",
    "        out_alph, out_char = net(img)\n",
    "        _, pred_alph = torch.max(out_alph, 1)\n",
    "        _, pred_char = torch.max(out_char, 1)\n",
    "        acc_alph(pred_alph, lables_alph)\n",
    "        acc_char(pred_char, labels_char)\n",
    "\n",
    "acc_alph.compute()\n",
    "acc_char.compute()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMa/JBmkZhbHyrT/CavceE5",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
